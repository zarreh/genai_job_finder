# GenAI Job Finder

A comprehensive job finder application that uses AI and web scraping technologies to collect, analyze, and find relevant job opportunities from LinkedIn.

## ğŸš€ Features

- **LinkedIn Job Scraping**: Automated job collection from LinkedIn using requests and BeautifulSoup
- **Web Frontend**: Streamlit-based user interface for easy job searching
- **Database Storage**: SQLite database for storing job listings and tracking parsing runs
- **Job Analysis**: Jupyter notebooks for analyzing collected job data
- **AI Integration**: Ready for AI-powered job matching and analysis (using LangChain, OpenAI, Ollama)
- **Vector Store Support**: Chroma vector database for semantic job search
- **Modular Architecture**: Clean, organized codebase with separate modules

## ğŸ“‹ Requirements

- Python 3.12+
- Poetry (for dependency management)

## ğŸ› ï¸ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/zarreh/genai_job_finder.git
cd genai_job_finder
```

2. **Install dependencies using Poetry:**
```bash
poetry install
```

3. **Activate the virtual environment:**
```bash
poetry shell
```

## ğŸ—ï¸ Project Structure

```
genai_job_finder/
â”œâ”€â”€ genai_job_finder/           # Main package
â”‚   â”œâ”€â”€ __init__.py            # Package exports
â”‚   â”œâ”€â”€ frontend/              # Streamlit web frontend
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py            # Main Streamlit application
â”‚   â”‚   â”œâ”€â”€ config.py         # Frontend configuration
â”‚   â”‚   â”œâ”€â”€ run.py            # Application launcher
â”‚   â”‚   â””â”€â”€ README.md         # Frontend documentation
â”‚   â”œâ”€â”€ linkedin_parser/       # LinkedIn job scraping module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py         # Job and JobRun data models
â”‚   â”‚   â”œâ”€â”€ parser.py         # Main LinkedIn parser
â”‚   â”‚   â”œâ”€â”€ database.py       # Database operations
â”‚   â”‚   â””â”€â”€ config.py         # Parser configuration
â”‚   â””â”€â”€ legacy/               # Legacy scraping code
â”œâ”€â”€ notebooks/                # Jupyter notebooks for analysis
â”‚   â””â”€â”€ job_analysis.ipynb   # Job data analysis notebook
â”œâ”€â”€ data/                     # Database and data files
â”‚   â””â”€â”€ jobs.db              # SQLite database
â”œâ”€â”€ example_usage.py          # Example script to run the parser
â”œâ”€â”€ pyproject.toml           # Poetry configuration
â””â”€â”€ README.md
```

## ğŸ¯ Usage

### Web Frontend (Recommended)

Launch the Streamlit web application for an easy-to-use interface:

```bash
# Option 1: Using the launch script
./run_frontend.sh

# Option 2: Using Poetry directly
poetry run streamlit run genai_job_finder/frontend/app.py
```

The application will be available at `http://localhost:8501`

**Features:**
- Search jobs by title, keywords, and location
- Filter by posting date (24h, week, month)
- View results in paginated tables (15 jobs per page)
- Download results as CSV files
- Real-time search progress indicators

### Command Line Usage

Use the example script to collect job data:

```bash
poetry run python example_usage.py
```

**Options:**
1. **Parse from config** - Uses predefined search parameters
2. **Custom search** - Enter your own keywords and location
3. **View parsing history** - See recent parsing runs

### Example Custom Search

```python
from genai_job_finder import LinkedInJobParser, DatabaseManager

# Initialize database and parser
db = DatabaseManager("data/jobs.db")
parser = LinkedInJobParser(database=db)

# Parse jobs
jobs = parser.parse_jobs(
    search_query="senior data scientist",
    location="San Francisco",
    max_pages=3
)

print(f"Found {len(jobs)} jobs")
```

### Analyzing Job Data

Open the Jupyter notebook for detailed job analysis:

```bash
jupyter notebook notebooks/job_analysis.ipynb
```

The notebook provides:
- Total job counts and statistics
- Top 10 most recent jobs with details
- Company and location distribution
- Salary information analysis
- Remote work statistics

## ğŸ“Š Data Models

### Job Model
```python
@dataclass
class Job:
    job_id: str
    title: str
    company: str
    location: str
    description: str
    posted_date: Optional[datetime]
    salary_range: Optional[str]
    job_type: Optional[JobType]
    experience_level: Optional[ExperienceLevel]
    remote_option: bool
    easy_apply: bool
    linkedin_url: Optional[str]
    # ... additional fields
```

### JobRun Model
```python
@dataclass
class JobRun:
    id: Optional[int]
    run_date: datetime
    search_query: str
    location_filter: str
    job_count: int
    status: str
    # ... additional tracking fields
```

## ğŸ”§ Configuration

### Search Parameters

Configure search parameters in `genai_job_finder/legacy/config.py`:

```python
LINKEDIN_JOB_SEARCH_PARAMS = [
    {
        "keywords": "senior data scientist",
        "location": "San Antonio",
        "f_TPR": "r86400",  # last 24 hours
        "remote": False,
    },
    # Add more search configurations...
]
```

### Time Filters
- `r86400`: Last 24 hours
- `r604800`: Last 7 days  
- `r2592000`: Last 30 days

## ğŸ¤– AI Integration

The project includes AI frameworks for future enhancements:

- **LangChain**: For building AI applications
- **LangGraph**: For complex AI workflows
- **OpenAI Integration**: GPT models support
- **Ollama Support**: Local LLM integration
- **Chroma Vector Store**: Semantic job search

## ğŸ“ˆ Database Schema

### Tables

1. **jobs** - Stores individual job listings
2. **job_runs** - Tracks parsing sessions

### Key Features
- Automatic duplicate prevention
- Run tracking and status monitoring
- Comprehensive job metadata storage
- SQLite for lightweight, portable database

## ğŸš¦ Getting Started

1. **First Run:**
```bash
poetry run python example_usage.py
```
Choose option 1 to parse jobs using default configuration.

2. **View Results:**
Open `notebooks/job_analysis.ipynb` and run all cells to see your collected job data.

3. **Customize:**
- Modify search parameters in the config file
- Add new search terms and locations
- Adjust the number of pages to parse

## ğŸ” Example Output

```
Total jobs in database: 212
Recent parsing runs:
- Run 7: senior data scientist (21 jobs) - completed
- Run 6: senior data scientist (12 jobs) - completed

Top Companies:
â€¢ EY: 3 jobs
â€¢ Lensa: 2 jobs
â€¢ USAA: 2 jobs

Remote Jobs: 45% of listings
Easy Apply: 67% of listings
```

## ğŸ›¡ï¸ Rate Limiting & Best Practices

- Built-in delays between requests (2-4 seconds)
- Respectful scraping practices
- User-agent rotation
- Error handling and retry logic

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ License

This project is for educational and personal use. Please respect LinkedIn's Terms of Service when using this tool.

## ğŸ› Troubleshooting

### Common Issues

1. **Import Errors**: Make sure you're in the Poetry environment (`poetry shell`)
2. **Database Locked**: Close any open database connections
3. **No Jobs Found**: Check your search parameters and network connection

### Getting Help

- Check the example usage script for proper usage patterns
- Review the Jupyter notebook for data analysis examples
- Ensure all dependencies are installed with `poetry install`

## ğŸ“Š Metrics

- **Total Jobs Collected**: 212+ and growing
- **Success Rate**: High reliability with error handling
- **Performance**: ~7 jobs per page, 3-4 seconds between requests

---

**Note**: This tool is designed for personal job search assistance. Please use responsibly and in accordance with LinkedIn's Terms of Service.
