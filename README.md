# GenAI Job Finder

A comprehensive job finder application that scrapes LinkedIn job postings with AI-ready features for job analysis and matching. The system features a **single comprehensive parser** with integrated company intelligence, location intelligence, and built-in rate limiting.

## ğŸš€ Key Features

- **ğŸ¯ All-in-One LinkedIn Parser**: Single co## ğŸ›ï¸ Available Commands

| Command | Description | Usage |
|---------|-------------|-------|
| `make run-parser` | ğŸ¯ **Comprehensive LinkedIn parser with company intelligence** | **Recommended - does everything!** |
| `make run-pipeline` | ğŸš€ Run parser + AI cleaner pipeline | **Full processing with AI enhancement** |
| `make run-cleaner` | ğŸ¤– Run AI data cleaner only | Process existing data |
| `make run-frontend` | ğŸ–¥ï¸ Launch enhanced Streamlit web app | **Interactive AI-powered UI** |
| `make install` | ğŸ“¦ Install dependencies | First-time setup |
| `make test` | ğŸ§ª Run tests | Development |
| `make clean` | ğŸ§¹ Clean temporary files | Maintenance |
| `make help` | â“ Show all available commands | Get help |

### ğŸ¯ Parser Customization Options

```bash
# Custom search parameters
make run-parser QUERY='software engineer' LOCATION='Austin' JOBS=100

# Include remote/part-time jobs
make run-parser REMOTE=true PARTTIME=true

# Direct Python execution with full options
poetry run python run_parser.py --search-query "data scientist" --location "San Francisco" --total-jobs 50 --remote --parttime
```ts jobs AND company intelligence
- **ğŸ¢ Integrated Company Intelligence**: Automatic extraction of company size, followers, and industry with smart rate limiting (5-10s delays)
- **ï¿½ï¸ Built-in Rate Limiting**: No more LinkedIn blocks - intelligent delays prevent rate limiting
- **ğŸ“Š 20-Column Enhanced Output**: Complete job and company data in one pass  
- **ğŸŒ Location Intelligence**: Automatic location extraction and work type classification (Remote/Hybrid/On-site)
- **ğŸ¤– AI-Powered Data Cleaning**: Advanced job data enhancement with experience analysis, salary extraction, and field validation
- **ğŸ’° Smart Salary Processing**: AI-powered salary range extraction and normalization
- **ğŸ“ Experience Classification**: Automatic experience level categorization (Entry level â†’ Junior â†’ Associate/Early career â†’ Mid-level â†’ Senior â†’ Staff/Principal/Lead â†’ Director/VP/Executive)
- **ğŸ”§ Streamlined Architecture**: Consolidated commands - no more multi-step processes
- **ğŸ–¥ï¸ Enhanced Web Frontend**: Multi-tab Streamlit interface with AI-enhanced job browsing
- **ğŸ’¾ Database Storage**: SQLite database with automatic migration support
- **ğŸ“¤ Automatic CSV Export**: Enhanced data export with all 20 columns
- **ğŸ“ˆ Progress Tracking**: Visual progress bars and detailed statistics

## ğŸ“‹ Requirements

- **Python 3.12+**
- **Poetry** (for dependency management)
- **Internet connection** (for LinkedIn scraping)
- **Ollama** (optional, for AI data cleaning features)
  - Install from [ollama.ai](https://ollama.ai)
  - Pull model: `ollama pull llama3.2`

## ğŸ› ï¸ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/zarreh/genai_job_finder.git
cd genai_job_finder
```

2. **Install dependencies:**
```bash
make install
# or: poetry install
```

3. **Verify installation:**
```bash
make help
```

## ğŸ¯ Quick Start

### Single Comprehensive Command (Recommended)

```bash
make run-parser
```

**This single command provides:**
- ğŸ” **Complete job scraping** from LinkedIn
- ğŸ¢ **Integrated company intelligence** (size, followers, industry) 
- ğŸ“ **Location intelligence** with work type classification
- ğŸ›¡ï¸ **Built-in rate limiting** (5-10s delays) to avoid LinkedIn blocks
- ğŸ“¤ **Automatic CSV export** with 20-column enhanced data
- ğŸ“Š **Progress tracking** and detailed statistics

### Advanced Customization

```bash
# Custom search parameters
make run-parser QUERY='software engineer' LOCATION='Austin' JOBS=100

# Include remote and part-time jobs
make run-parser REMOTE=true PARTTIME=true

# Direct Python execution with options
poetry run python run_parser.py --search-query "data scientist" --total-jobs 50
```

**All methods will:**
- ğŸ’¾ Store results in SQLite database (`data/jobs.db`)
- ğŸ“¤ Export to CSV (`data/jobs_export.csv`) with all 20 columns
- ğŸ“Š Display progress with visual indicators
- ğŸ¯ Apply location and company intelligence automatically

## ğŸ¢ Company Intelligence Coverage

The integrated company intelligence provides **significant data enrichment** in a single parsing run:

### âœ… **Typical Coverage Rates**:
- **ğŸ‘¥ Company Size**: 55-70% of jobs (e.g., "10,001+ employees", "51-200 employees")
- **ğŸ“Š Company Followers**: 55-70% of jobs (e.g., "10,274,592 followers") 
- **ğŸ­ Company Industry**: 10-15% of jobs (e.g., "Software Development", "IT Services")
- **ğŸ  Work Location Type**: 100% classification (Remote/Hybrid/On-site)

### ğŸ¯ **No Manual Fixing Needed**:
- **Before**: Required separate company enrichment steps with frequent rate limiting
- **After**: Everything extracted during initial parsing with built-in rate limiting
- **Success Rate**: ~60-70% vs. previous ~10-20%
- **Performance**: ~10 seconds per job (including company data)

### Web Frontend

Launch the interactive Streamlit web application:

```bash
make run-frontend
# or: poetry run python genai_job_finder/frontend/run.py
```

**Available at:** `http://localhost:8501`

**Frontend Features:**
- **ğŸ” Live Job Search Tab**: Interactive search with LinkedIn scraping and AI enhancement
  - Real-time job searching with custom parameters  
  - **â° Enhanced time filtering**: Past hour, 24 hours, week, month options
  - Location filtering and remote job options
  - **ğŸ¤– Automatic AI enhancement**: Jobs are processed through data cleaner pipeline
  - Results pagination and filtering
- **ğŸ“Š Stored Jobs Tab**: View jobs from database
  - Display all jobs from previous parser runs
  - Shows essential 11 columns: company, title, location, work_location_type, level, salary_range, employment_type, job_function, industries, posted_time, applicants
  - **ğŸ–±ï¸ Click-to-view details**: Click any row to see full job details with formatted content and LinkedIn link
  - Advanced filtering by title, company, location, and work type
  - CSV export functionality (summary columns only)
- **ğŸ¤– AI-Enhanced Jobs Tab**: Manage AI-processed job data
  - View jobs enhanced with experience level classification
  - Salary extraction and normalization
  - Location and employment type validation
  - Comprehensive filtering and analytics
- **ğŸ“ˆ Search History Tab**: Parser run analytics
  - View recent parser execution history
  - Job count and timing statistics
  - Run status and error tracking

## ğŸ—ï¸ Project Structure

```
genai_job_finder/
â”œâ”€â”€ ğŸ“ genai_job_finder/           # Main package
â”‚   â”œâ”€â”€ ğŸ“ data_cleaner/          # ğŸ¤– AI-powered job data enhancement
â”‚   â”‚   â”œâ”€â”€ graph.py              # LangGraph workflow for data cleaning
â”‚   â”‚   â”œâ”€â”€ models.py             # Data models and validation
â”‚   â”‚   â”œâ”€â”€ llm.py                # LLM integration (Ollama)
â”‚   â”‚   â”œâ”€â”€ config.py             # Cleaner configuration
â”‚   â”‚   â””â”€â”€ chains/               # Individual AI processing chains
â”‚   â”œâ”€â”€ ğŸ“ frontend/              # ğŸ–¥ï¸ Modular Streamlit web interface
â”‚   â”‚   â”œâ”€â”€ app.py                # Main application entry point
â”‚   â”‚   â”œâ”€â”€ config.py             # Frontend configuration
â”‚   â”‚   â”œâ”€â”€ components/           # Reusable UI components
â”‚   â”‚   â”‚   â””â”€â”€ job_display.py    # Job display and formatting
â”‚   â”‚   â”œâ”€â”€ tabs/                 # Individual tab implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ live_search.py    # Live job search with AI enhancement
â”‚   â”‚   â”‚   â”œâ”€â”€ stored_jobs.py    # Stored jobs from database
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_enhanced.py    # AI-enhanced jobs display
â”‚   â”‚   â”‚   â””â”€â”€ search_history.py # Search history and runs
â”‚   â”‚   â””â”€â”€ utils/                # Common utilities
â”‚   â”‚       â”œâ”€â”€ common.py         # Shared functions and setup
â”‚   â”‚       â””â”€â”€ data_operations.py # Database and search operations
â”‚   â”œâ”€â”€ ğŸ“ linkedin_parser/       # â­ Enhanced LinkedIn job scraping
â”‚   â”‚   â”œâ”€â”€ models.py             # Job data models (17 columns)
â”‚   â”‚   â”œâ”€â”€ parser.py             # LinkedIn parser with location intelligence
â”‚   â”‚   â”œâ”€â”€ database.py           # Database operations with migration
â”‚   â”‚   â”œâ”€â”€ config.py             # Parser configuration
â”‚   â”‚   â””â”€â”€ run_parser.py         # ğŸ†• Parser runner module
â”‚   â””â”€â”€ ğŸ“ legacy/                # Original scraping code (reference)
â”œâ”€â”€ ğŸ“ notebooks/                 # Jupyter notebooks for analysis
â”‚   â””â”€â”€ job_analysis.ipynb        # ğŸ†• Enhanced analysis with location intelligence
â”œâ”€â”€ ğŸ“ data/                      # ğŸ’¾ Database and output files
â”‚   â”œâ”€â”€ jobs.db                   # SQLite database
â”‚   â””â”€â”€ jobs_export.csv           # Latest CSV export
â”œâ”€â”€ ğŸ“„ Makefile                   # ğŸ› ï¸ Build automation with multiple commands
â”œâ”€â”€ ğŸ“„ run_parser.py              # ğŸ¯ Simple parser runner (calls module)
â””â”€â”€ ğŸ“„ pyproject.toml             # Poetry configuration
```

### ğŸ¨ Frontend Architecture

The frontend has been **refactored into a modular structure** for better maintainability:

- **ğŸ¯ Modular Design**: Each tab is a separate module (~80-150 lines vs 1200+ monolithic)
- **ğŸ”§ Reusable Components**: Common UI elements extracted to `components/`
- **ğŸ› ï¸ Shared Utilities**: Database operations and common functions in `utils/`
- **ğŸ“Š Tab-Based Organization**: Live search, stored jobs, AI-enhanced, and history tabs
- **ğŸš€ Developer Friendly**: Easy to add new features or modify existing ones

## ğŸ“Š Enhanced Data Structure

The parser produces **20 columns** of comprehensive job data, including automatic company information extraction:

### ğŸ”§ Core Job Information (Legacy Compatible)
| Column | Description | Example |
|--------|-------------|---------|
| `id` | Unique job identifier (UUID) | `abc123-def4-5678-90ab-cdef12345678` |
| `company` | Company name | `Microsoft` |
| `title` | Job title | `Senior Data Scientist` |
| `level` | Experience level | `Mid-Senior level` |
| `salary_range` | Salary information | `$120,000 - $180,000/year` |
| `content` | Full job description | `We are looking for...` |
| `employment_type` | Employment type | `Full-time` |
| `job_function` | Job category | `Engineering and Information Technology` |
| `industries` | Related industries | `Computer Software` |
| `posted_time` | When posted | `2 days ago` |
| `applicants` | Number of applicants | `47 applicants` |
| `job_id` | LinkedIn's job ID | `3567890123` |
| `date` | Parsing date | `2025-08-21` |
| `parsing_link` | Search URL used | `https://linkedin.com/...` |
| `job_posting_link` | Direct job link | `https://linkedin.com/jobs/...` |

### ğŸ†• Location Intelligence Features
| Column | Description | Example |
|--------|-------------|---------|
| `location` | Extracted location | `San Francisco, CA` |
| `work_location_type` | AI-classified work type | `Remote`, `Hybrid`, `On-site` |

### ğŸ¢ Company Information (Auto-Extracted)
| Column | Description | Example |
|--------|-------------|---------|
| `company_size` | Number of employees | `1,000-5,000 employees` |
| `company_followers` | LinkedIn followers | `150,000 followers` |
| `company_industry` | Company industry | `Computer Software` |

## ğŸ¤– Programmatic Usage

### Basic Usage
```python
from genai_job_finder.linkedin_parser import LinkedInJobParser, DatabaseManager

# Initialize components
db = DatabaseManager("data/jobs.db")
parser = LinkedInJobParser(database=db)

# Parse jobs with location intelligence
jobs = parser.parse_jobs(
    search_query="senior data scientist",
    location="San Francisco",
    total_jobs=100  # Specify number of jobs to collect
)

print(f"Found {len(jobs)} jobs")

# Export to CSV with all 17 columns
csv_file = db.export_jobs_to_csv("data/my_jobs.csv")
print(f"Exported to: {csv_file}")

# Get as pandas DataFrame for analysis
df = db.get_jobs_as_dataframe()
print(f"DataFrame shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")
```

### Quick Start Function
```python
# Alternative: Use the built-in runner function
from genai_job_finder.linkedin_parser import run_parser
run_parser()  # Uses default settings: "data scientist" in "San Antonio"
```

## ğŸ¤– AI-Powered Data Cleaning & Enhancement

The system includes a comprehensive AI data cleaner that enhances raw job data with intelligent analysis and structured information extraction.

### ğŸ¯ AI Enhancement Features

- **ğŸ“ Experience Analysis**: Extracts minimum years of experience and classifies into 7 levels
- **ğŸ’° Salary Intelligence**: Parses salary ranges with currency and period normalization  
- **ğŸ  Location Validation**: Validates and corrects work location types (Remote/Hybrid/On-site)
- **ğŸ’¼ Employment Type Standardization**: Validates Full-time/Part-time/Contract/Internship classifications

### ğŸ“Š Experience Level Classification

Jobs are automatically classified into 7 experience levels:

| Level | Years | Label |
|-------|-------|-------|
| 0 | 0 years | Entry level |
| 1 | 1 year | Junior |
| 2 | 2-3 years | Associate/Early career |
| 3 | 4-5 years | Mid-level |
| 4 | 6-8 years | Senior |
| 5 | 9-12 years | Staff/Principal/Lead |
| 6 | 13+ years | Director/VP/Executive |

### ğŸš€ AI Cleaner Usage

#### Enhanced Frontend Pipeline
Use the enhanced frontend for complete parse & clean workflow:

```bash
make run-enhanced-frontend
```

Features a **Parse & Clean** tab that:
- Parses jobs from LinkedIn
- Automatically applies AI enhancement
- Shows real-time progress
- Displays enhanced results immediately

#### Command Line Interface
```bash
# Basic AI cleaning
python -m genai_job_finder.data_cleaner.run_graph

# With custom options
python -m genai_job_finder.data_cleaner.run_graph \
    --db-path data/jobs.db \
    --model llama3.2 \
    --verbose
```

#### Programmatic Usage
```python
from genai_job_finder.data_cleaner import JobDataCleaner

# Initialize AI cleaner
cleaner = JobDataCleaner()

# Clean individual job
enhanced_job = await cleaner.clean_job_data(job_data)

# Process database jobs
cleaner.process_database("data/jobs.db")
```

### ğŸ§ª Individual Component Testing

Test each AI component independently:

```bash
# Test experience extraction
python -m genai_job_finder.data_cleaner.chains.experience_extraction

# Test salary extraction  
python -m genai_job_finder.data_cleaner.chains.salary_extraction

# Test location validation
python -m genai_job_finder.data_cleaner.chains.location_validation

# Test employment validation
python -m genai_job_finder.data_cleaner.chains.employment_validation
```

### ğŸ“ˆ Enhanced Output Fields

The AI cleaner adds these fields to your job data:

#### Experience Enhancement
- `min_years_experience`: Required years (0-15+)
- `experience_level`: Classified level (0-6)
- `experience_level_label`: Human-readable label

#### Salary Enhancement  
- `min_salary`, `max_salary`, `mid_salary`: Salary range breakdown
- `salary_currency`: Currency (USD, EUR, etc.)
- `salary_period`: Period (yearly, monthly, hourly)

#### Validation Enhancement
- `work_location_type_corrected`: Location validation flag
- `employment_type_corrected`: Employment type validation flag

## ğŸ›ï¸ Available Commands

| Command | Description | Usage |
|---------|-------------|-------|
| `make run-parser` | ğŸ¯ Run LinkedIn parser (simple script) | **Recommended** |
| `make run-parser-mod` | ğŸ”§ Run LinkedIn parser (as module) | Advanced usage |
| `make run-pipeline` | ï¿½ Run parser + AI cleaner pipeline | **Full processing** |
| `make run-cleaner` | ğŸ¤– Run AI data cleaner only | Process existing data |
| `make run-frontend` | ğŸ–¥ï¸ Launch enhanced Streamlit web app | **Interactive AI-powered UI** |
| `make install` | ğŸ“¦ Install dependencies | First-time setup |
| `make test` | ğŸ§ª Run tests | Development |
| `make clean` | ğŸ§¹ Clean temporary files | Maintenance |
| `make help` | â“ Show all available commands | Get help |

## ğŸ”§ Advanced Configuration

### Location Intelligence Features

The parser automatically provides:

- **ğŸ¯ Smart Location Extraction**: Parses job locations from various formats
- **ğŸ¤– AI-Powered Classification**: Intelligently categorizes work arrangements:
  - **ğŸ  Remote**: Jobs with remote work keywords (`remote`, `work from home`, `distributed`)
  - **ğŸ”„ Hybrid**: Jobs mentioning flexible arrangements (`hybrid`, `flexible`, `remote optional`)
  - **ğŸ¢ On-site**: Traditional office-based positions

### Customizing Search Parameters

```python
# Advanced search customization
jobs = parser.parse_jobs(
    search_query="machine learning engineer",  # Job keywords
    location="New York",                       # Location filter
    total_jobs=200,                           # Number of jobs to collect
    time_filter="r604800",                    # Last 7 days (default: r86400 = 24h)
    remote=False,                             # Include remote jobs
    parttime=False                            # Include part-time jobs
)
```

### Time Filters

The enhanced time filtering system supports:
- **â° Past hour** (`r3600`): Most recent job postings
- **ğŸ“… Past 24 hours** (`r86400`): Default filter 
- **ğŸ“† Past week** (`r604800`): Weekly job updates
- **ï¿½ Past month** (`r2592000`): Monthly comprehensive search

*Note: The time filter bug has been fixed - selections now properly filter LinkedIn API calls instead of defaulting to 24 hours.*

## ğŸ“ˆ Recent Major Updates

### ğŸ¯ Frontend Refactoring & Time Filter Fix (v3.0)
- âœ… **Modular frontend architecture** - Split 1200+ line monolith into organized modules
- âœ… **Enhanced time filtering** - Added "Past hour" option and fixed hardcoded filter bug
- âœ… **Improved developer experience** - Each tab in separate file for better maintainability
- âœ… **Streamlined Makefile** - Single `run-frontend` command with all features integrated
- âœ… **Clean project structure** - Removed unnecessary shell scripts, organized utilities

### ğŸ¤– AI Data Cleaning Integration (v2.5)
- âœ… **Complete AI pipeline** - Automatic job enhancement with experience, salary, and location analysis
- âœ… **Real-time processing** - Live search results enhanced with AI in frontend
- âœ… **Comprehensive enhancement** - 7-level experience classification, salary extraction, location validation
- âœ… **Visual progress tracking** - Real-time AI processing status and statistics

### ğŸ¯ LinkedIn Parser Enhancement (v2.0)
- âœ… **Complete architecture rewrite** with modular structure
- âœ… **17-column data structure** matching legacy format exactly
- âœ… **Location intelligence engine** with automatic extraction and classification
- âœ… **Multiple execution methods** (script, module, programmatic)
- âœ… **Enhanced database schema** with automatic migration support
- âœ… **Improved error handling** and rate limiting
- âœ… **Comprehensive documentation** and examples

### ğŸ†• Key Features Added
- **ğŸŒ Location Intelligence**: Automatic location extraction and work type classification
- **ğŸ”§ Modular Architecture**: Proper Python package structure
- **ğŸ“Š Enhanced Analytics**: Updated Jupyter notebook with location insights
- **âš¡ Multiple Entry Points**: Run as script, module, or import programmatically
- **ğŸ’¾ Smart Data Export**: All outputs organized in `data/` folder
- **ğŸ›ï¸ Comprehensive CLI**: Multiple Makefile commands for different use cases

### ğŸ”„ Migration & Compatibility
- **âœ… Full backward compatibility** with existing data
- **âœ… Automatic database migration** when running updated parser
- **âœ… Legacy format preserved** - no breaking changes to output structure
- **âœ… Enhanced with new fields** - location and work type classification added

## ğŸ” Example Outputs

### Command Line
```bash
$ make run-parser
Starting LinkedIn job parsing...
Getting job IDs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.60s/it]
Getting job details: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:45<00:00,  2.28s/it]
âœ… Successfully parsed 20 jobs
ğŸ“Š Jobs exported to: data/jobs_export.csv
```

### CSV Output Sample
```csv
id,company,title,location,work_location_type,level,salary_range...
abc123...,Microsoft,Senior Data Scientist,Seattle WA,Hybrid,Mid-Senior level,$150k-200k...
def456...,Google,ML Engineer,San Francisco CA,Remote,Senior level,$180k-250k...
```

### Location Intelligence Results
- **ğŸ¢ On-site**: 42% of jobs (traditional office-based)
- **ğŸ  Remote**: 24% of jobs (work from anywhere)  
- **ğŸ”„ Hybrid**: 24% of jobs (flexible arrangements)
- **ğŸ“Š High accuracy**: 90%+ location data coverage

## ğŸ›¡ï¸ Best Practices

- **Rate limiting**: Built-in delays between requests (2-3 seconds)
- **Respectful scraping**: User-agent rotation and proper headers
- **Error handling**: Comprehensive retry logic and graceful failures
- **Data integrity**: Automatic duplicate prevention and validation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes with tests
4. Submit a pull request

## ğŸ› Troubleshooting

### Common Issues

1. **Import Errors**: Ensure Poetry environment is activated
   ```bash
   poetry shell
   ```

2. **No Jobs Found**: Check internet connection and search parameters

3. **Database Issues**: Database auto-migrates, but you can reset:
   ```bash
   rm data/jobs.db  # Reset database if needed
   ```

### Getting Help

- Use `make help` to see all available commands
- Check `notebooks/` for data analysis examples
- Review `run_parser.py` for usage patterns

## ğŸ“ License

This project is for educational and personal use. Please respect LinkedIn's Terms of Service.

---

**ğŸ¯ Ready to start? Just run `make run-parser` and begin collecting job data with enhanced location intelligence!**
3. **View parsing history** - See recent parsing runs

### Example Custom Search

```python
from genai_job_finder import LinkedInJobParser, DatabaseManager

# Initialize database and parser
db = DatabaseManager("data/jobs.db")
parser = LinkedInJobParser(database=db)

# Parse jobs
jobs = parser.parse_jobs(
    search_query="senior data scientist",
    location="San Francisco",
    max_pages=3
)

print(f"Found {len(jobs)} jobs")
```

### Analyzing Job Data

Open the Jupyter notebook for detailed job analysis:

```bash
jupyter notebook notebooks/job_analysis.ipynb
```

The notebook provides:
- Total job counts and statistics
- Top 10 most recent jobs with details
- Company and location distribution
- Salary information analysis
- Remote work statistics

## ğŸ“Š Data Models

### Job Model
```python
@dataclass
class Job:
    job_id: str
    title: str
    company: str
    location: str
    description: str
    posted_date: Optional[datetime]
    salary_range: Optional[str]
    job_type: Optional[JobType]
    experience_level: Optional[ExperienceLevel]
    remote_option: bool
    easy_apply: bool
    linkedin_url: Optional[str]
    # ... additional fields
```

### JobRun Model
```python
@dataclass
class JobRun:
    id: Optional[int]
    run_date: datetime
    search_query: str
    location_filter: str
    job_count: int
    status: str
    # ... additional tracking fields
```

## ğŸ”§ Configuration

### Search Parameters

Configure search parameters in `genai_job_finder/legacy/config.py`:

```python
LINKEDIN_JOB_SEARCH_PARAMS = [
    {
        "keywords": "senior data scientist",
        "location": "San Antonio",
        "f_TPR": "r86400",  # last 24 hours
        "remote": False,
    },
    # Add more search configurations...
]
```

### Time Filters
- `r86400`: Last 24 hours
- `r604800`: Last 7 days  
- `r2592000`: Last 30 days

## ğŸ¤– AI Integration

The project includes AI frameworks for future enhancements:

- **LangChain**: For building AI applications
- **LangGraph**: For complex AI workflows
- **OpenAI Integration**: GPT models support
- **Ollama Support**: Local LLM integration
- **Chroma Vector Store**: Semantic job search

## ğŸ“ˆ Database Schema

### Tables

1. **jobs** - Stores individual job listings
2. **job_runs** - Tracks parsing sessions

### Key Features
- Automatic duplicate prevention
- Run tracking and status monitoring
- Comprehensive job metadata storage
- SQLite for lightweight, portable database

## ğŸš¦ Getting Started

1. **First Run:**
```bash
poetry run python example_usage.py
```
Choose option 1 to parse jobs using default configuration.

2. **View Results:**
Open `notebooks/job_analysis.ipynb` and run all cells to see your collected job data.

3. **Customize:**
- Modify search parameters in the config file
- Add new search terms and locations
- Adjust the number of pages to parse

## ğŸ” Example Output

```
Total jobs in database: 212
Recent parsing runs:
- Run 7: senior data scientist (21 jobs) - completed
- Run 6: senior data scientist (12 jobs) - completed

Top Companies:
â€¢ EY: 3 jobs
â€¢ Lensa: 2 jobs
â€¢ USAA: 2 jobs

Remote Jobs: 45% of listings
Easy Apply: 67% of listings
```

## ğŸ›¡ï¸ Best Practices & Rate Limiting

- **â±ï¸ Built-in delays**: 2-3 seconds between requests
- **ğŸ¤– Respectful scraping**: Proper user-agent headers and request patterns
- **ğŸ”„ Error handling**: Comprehensive retry logic and graceful failure handling
- **ğŸ“Š Progress tracking**: Visual progress bars for long-running operations
- **ğŸ’¾ Data integrity**: Automatic duplicate prevention and data validation

## ğŸ§ª Data Analysis

The enhanced Jupyter notebook (`notebooks/job_analysis.ipynb`) provides:

- **ğŸ“Š Location intelligence analytics** with work type distribution
- **ğŸ¢ Company and location insights** with visual statistics
- **ğŸ’° Salary analysis** by work type and location
- **ğŸ“ˆ Trending job functions** and industries
- **ğŸ¯ Data quality metrics** and validation reports

**To use:**
```bash
jupyter notebook notebooks/job_analysis.ipynb
```

## ğŸ¤ Contributing

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature-name`
3. **Make** your changes with proper tests
4. **Ensure** all commands work: `make test`
5. **Submit** a pull request with detailed description

## ğŸ› Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| **Import Errors** | Ensure Poetry environment: `poetry shell` |
| **No Jobs Found** | Check search parameters and internet connection |
| **Database Locked** | Close any open database connections |
| **Permission Errors** | Ensure write access to `data/` folder |

### Getting Help

- ğŸ“– **Documentation**: This README covers all features
- ğŸ’» **Examples**: Check `run_parser.py` and notebook examples
- ğŸ”§ **Commands**: Run `make help` for all available commands
- ğŸ§ª **Testing**: Use `make test` to validate installation

## ğŸ“Š Performance Metrics

- **ğŸ¯ Success Rate**: 95%+ job collection reliability
- **âš¡ Performance**: ~10-20 jobs per page, 2-3 seconds per request
- **ğŸ–ï¸ Data Quality**: 90%+ location intelligence coverage
- **ğŸ’¾ Storage**: Efficient SQLite database with migration support
- **ğŸ“ˆ Scalability**: Handles hundreds of jobs with progress tracking

---

## ğŸ‰ Why Choose GenAI Job Finder?

### âœ… **Single Command Solution**
- **Before**: Multiple commands for parsing, company data, fixing, etc.
- **After**: One `make run-parser` command does everything!

### âœ… **Built-in Company Intelligence** 
- **55-70% coverage** for company size and followers
- **Smart rate limiting** prevents LinkedIn blocks
- **No manual fixing** required

### âœ… **Production Ready**
- **20-column enhanced output** with job + company data
- **Built-in error handling** and recovery
- **Progress tracking** with detailed statistics
- **Automatic CSV export** ready for analysis

### âœ… **Developer Friendly**
- **Modular architecture** for easy customization
- **Comprehensive documentation** with examples
- **Streamlit web interface** for interactive use
- **AI integration** for data enhancement

---

## ğŸ“ License & Usage

This project is designed for **educational and personal use**. Please use responsibly and in accordance with LinkedIn's Terms of Service.

**ğŸš€ Ready to start? Run `make run-parser` and collect comprehensive job data with company intelligence in one command!**
