# GenAI Job Finder

A comprehensive job finder application that scrapes LinkedIn job postings with AI-ready features for job analysis and matching. The system features a **separate company enrichment pipeline** with intelligent company data management and advanced frontend display capabilities.

## ğŸš€ Key Features

- **ï¿½ Separate Company Enrichment Pipeline**: Dedicated company information service with lookup-first approach to eliminate redundant parsing  
- **ï¿½ Optimized LinkedIn Parser**: Smart company handling with 3-5x performance improvement for existing companies
- **ğŸ›¡ï¸ Built-in Rate Limiting**: No more LinkedIn blocks - intelligent delays prevent rate limiting
- **ğŸ“Š Enhanced Frontend Display**: Separate "Company Info" column with rich metadata formatting (ğŸ­ Industry â€¢ ï¿½ Size â€¢ ï¿½â€ï¿½ Followers)
- **ğŸŒ Location Intelligence**: Automatic location extraction and work type classification (Remote/Hybrid/On-site)
- **ğŸ¤– AI-Powered Data Cleaning**: Advanced job data enhancement with experience analysis, salary extraction, and field validation
- **ğŸ’° Smart Salary Processing**: AI-powered salary range extraction and normalization
- **ğŸ“ Experience Classification**: Automatic experience level categorization (Entry level â†’ Junior â†’ Associate/Early career â†’ Mid-level â†’ Senior â†’ Staff/Principal/Lead â†’ Director/VP/Executive)
- **ğŸ–¥ï¸ Enhanced Web Frontend**: Multi-tab Streamlit interface with dedicated Company Info display
- **ğŸ’¾ Database Storage**: SQLite database with separate companies table and foreign key relationships
- **ğŸ“¤ Automatic CSV Export**: Enhanced data export with comprehensive company information
- **ğŸ“ˆ Progress Tracking**: Visual progress bars and detailed statistics

## ğŸ“‹ Requirements

- **Python 3.12+**
- **Poetry** (for dependency management)
- **Internet connection** (for LinkedIn scraping)
- **Ollama** (optional, for AI data cleaning features)
  - Install from [ollama.ai](https://ollama.ai)
  - Pull model: `ollama pull llama3.2`

## ğŸ› ï¸ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/zarreh/genai_job_finder.git
cd genai_job_finder
```

2. **Install dependencies:**
```bash
make install
# or: poetry install
```

3. **Verify installation:**
```bash
make help
```

## ğŸ¯ Quick Start

### Single Comprehensive Command (Recommended)

```bash
make run-parser
```

**This optimized command provides:**
- ğŸ” **Complete job scraping** from LinkedIn
- ğŸ¢ **Smart company handling** (lookup-first approach) 
- ğŸ“ **Location intelligence** with work type classification
- ğŸ›¡ï¸ **Built-in rate limiting** (5-10s delays) to avoid LinkedIn blocks
- ğŸ“¤ **Automatic CSV export** with 21-column enhanced data
- ğŸ“Š **Progress tracking** and detailed statistics
- âš¡ **3-5x faster** for companies that already exist in database

### Company Enrichment Pipeline

```bash
# Show company enrichment statistics
make run-company-enrichment STATS=true

# Enrich companies that need additional data
make run-company-enrichment ENRICH=true

# Enrich specific company
make run-company-enrichment COMPANY='Microsoft'
```

**The company enrichment pipeline:**
- ğŸ“Š **Shows statistics** about company data coverage
- ğŸ”§ **Enriches missing data** for companies that need it
- âš¡ **Efficient processing** with built-in rate limiting
- ğŸ¯ **Independent operation** - can be run separately from job parsing

### Full Processing Pipeline

```bash
make run-pipeline
```

**Complete workflow:**
1. ğŸ¯ **Optimized job parsing** with smart company handling
2. ğŸ¢ **Company enrichment** for any missing company data  
3. ğŸ¤– **AI data cleaning** and enhancement
4. ğŸ“Š **Comprehensive statistics** and analytics

### Advanced Customization

```bash
# Custom search parameters
make run-parser QUERY='software engineer' LOCATION='Austin' JOBS=100

# Include remote and part-time jobs
make run-parser REMOTE=true PARTTIME=true

# Direct Python execution with options
poetry run python run_parser.py --search-query "data scientist" --total-jobs 50
```

**All methods will:**
- ğŸ’¾ Store results in SQLite database (`data/jobs.db`)
- ğŸ“¤ Export to CSV (`data/jobs_export.csv`) with all 21 columns
- ğŸ“Š Display progress with visual indicators
- ğŸ¯ Apply location and company intelligence automatically

## ğŸ¢ Company Intelligence & Optimization

The system features a **comprehensive separate company enrichment pipeline** that dramatically improves efficiency and data quality:

### âœ… **Separate Company Enrichment Service**:
- **ï¿½ï¸ Dedicated Pipeline**: Independent company enrichment service (`CompanyEnrichmentService`)
- **ğŸ” Lookup-First Approach**: Checks existing company data before attempting to parse
- **âš¡ Performance Optimization**: 3-5x faster processing for existing companies  
- **ğŸ”„ Smart Enrichment**: Only fetches company data when needed or missing
- **ğŸ“Š Independent Operation**: Can be run separately from job parsing

### âœ… **Enhanced Frontend Display**:
- **ğŸ“‹ Separate Company Info Column**: Dedicated column for rich company metadata
- **ğŸ¨ Rich Formatting**: Company info displayed as "ğŸ­ Industry â€¢ ğŸ‘¥ Size â€¢ ğŸ‘¨â€ğŸ’¼ Followers"
- **ğŸ”§ Clean Organization**: Company name and metadata separated for better readability
- **ğŸ“± Responsive Design**: Works across all frontend tabs (Live Search, Stored Jobs, AI Enhanced, Search History)

### âœ… **Typical Coverage Rates (After Enrichment)**:
- **ğŸ‘¥ Company Size**: 60-80% of companies (e.g., "10,001+ employees", "51-200 employees")
- **ğŸ“Š Company Followers**: 60-80% of companies with smart formatting (e.g., "467.3K followers", "29.5M followers") 
- **ğŸ­ Company Industry**: 15-25% of companies (e.g., "Software Development", "IT Services")
- **ğŸ”— Company LinkedIn URL**: 70-90% of companies (e.g., "https://www.linkedin.com/company/microsoft")
- **ğŸ  Work Location Type**: 100% classification (Remote/Hybrid/On-site)

### ğŸ¯ **Performance Improvements**:
- **Before**: Parsed company info for every job (~10 seconds per job)
- **After**: Only parses new companies (~2-3 seconds per job for existing companies)
- **Success Rate**: Maintains ~60-70% company data enrichment
- **Efficiency**: ~3-5x faster for repeat companies
- **Smart Caching**: Database-first lookup with intelligent fallback to parsing

### ğŸ¢ **Advanced Company Management**:
- **ğŸ—ƒï¸ Separate Database Table**: Companies stored in dedicated `companies` table
- **ğŸ”— Foreign Key Relationships**: Jobs reference companies via `company_id` 
- **ğŸ”„ Independent Pipeline**: Company enrichment runs separately from job parsing
- **ğŸ“Š Bulk Operations**: Bulk company enrichment with progress tracking
- **ğŸ“ˆ Statistics & Analytics**: Comprehensive company data coverage reporting

### Web Frontend

Launch the interactive Streamlit web application:

```bash
make run-frontend
# or: poetry run python genai_job_finder/frontend/run.py
```

**Available at:** `http://localhost:8501`

**Frontend Features:**
- **ğŸ” Live Job Search Tab**: Interactive search with LinkedIn scraping and AI enhancement
  - Real-time job searching with custom parameters  
  - **â° Enhanced time filtering**: Past hour, 24 hours, week, month options
  - Location filtering and remote job options
  - **ğŸ¤– Automatic AI enhancement**: Jobs are processed through data cleaner pipeline
  - **ğŸ¢ Company Info Column**: Dedicated column showing "ğŸ­ Industry â€¢ ğŸ‘¥ Size â€¢ ğŸ‘¨â€ğŸ’¼ Followers"
  - Results pagination and filtering
- **ğŸ“Š Stored Jobs Tab**: View jobs from database with enhanced company display
  - Display all jobs from previous parser runs
  - **ğŸ¢ Separate Company Info**: Company name and metadata in dedicated columns
  - Shows enhanced company information with emoji formatting
  - **ğŸ–±ï¸ Click-to-view details**: Click any row to see full job details with formatted content and LinkedIn link
  - Advanced filtering by title, company, location, and work type
  - CSV export functionality with company information
- **ğŸ¤– AI-Enhanced Jobs Tab**: Manage AI-processed job data with company enrichment
  - View jobs enhanced with experience level classification
  - **ğŸ¢ Company Intelligence**: Enhanced company data display across all enhanced jobs
  - Salary extraction and normalization
  - Location and employment type validation
  - Comprehensive filtering and analytics
- **ğŸ“ˆ Search History Tab**: Parser run analytics
  - View recent parser execution history
  - Job count and timing statistics
  - Run status and error tracking

## ğŸ—ï¸ Project Structure

```
genai_job_finder/
â”œâ”€â”€ ğŸ“ genai_job_finder/           # Main package
â”‚   â”œâ”€â”€ ğŸ“ data_cleaner/          # ğŸ¤– AI-powered job data enhancement
â”‚   â”‚   â”œâ”€â”€ graph.py              # LangGraph workflow for data cleaning
â”‚   â”‚   â”œâ”€â”€ models.py             # Data models and validation
â”‚   â”‚   â”œâ”€â”€ llm.py                # LLM integration (Ollama)
â”‚   â”‚   â”œâ”€â”€ config.py             # Cleaner configuration
â”‚   â”‚   â””â”€â”€ chains/               # Individual AI processing chains
â”‚   â”œâ”€â”€ ğŸ“ frontend/              # ğŸ–¥ï¸ Modular Streamlit web interface with company display
â”‚   â”‚   â”œâ”€â”€ app.py                # Main application entry point
â”‚   â”‚   â”œâ”€â”€ config.py             # Frontend configuration
â”‚   â”‚   â”œâ”€â”€ components/           # Reusable UI components
â”‚   â”‚   â”‚   â””â”€â”€ job_display.py    # Job display with separate Company Info column
â”‚   â”‚   â”œâ”€â”€ tabs/                 # Individual tab implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ live_search.py    # Live job search with AI enhancement
â”‚   â”‚   â”‚   â”œâ”€â”€ stored_jobs.py    # Stored jobs with company info display
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_enhanced.py    # AI-enhanced jobs with company data
â”‚   â”‚   â”‚   â””â”€â”€ search_history.py # Search history and runs
â”‚   â”‚   â””â”€â”€ utils/                # Common utilities
â”‚   â”‚       â”œâ”€â”€ common.py         # Shared functions and database path resolution
â”‚   â”‚       â””â”€â”€ data_operations.py # Database operations with company enrichment
â”‚   â”œâ”€â”€ ğŸ“ linkedin_parser/       # â­ Enhanced LinkedIn scraping with company pipeline
â”‚   â”‚   â”œâ”€â”€ models.py             # Job and Company data models
â”‚   â”‚   â”œâ”€â”€ parser.py             # LinkedIn parser with company integration
â”‚   â”‚   â”œâ”€â”€ company_enrichment.py # ğŸ†• Separate company enrichment service
â”‚   â”‚   â”œâ”€â”€ company_parser.py     # Company-specific parsing logic
â”‚   â”‚   â”œâ”€â”€ database.py           # Database operations with companies table
â”‚   â”‚   â”œâ”€â”€ config.py             # Parser configuration
â”‚   â”‚   â””â”€â”€ run_parser.py         # Parser runner module
â”‚   â””â”€â”€ ğŸ“ legacy/                # Original scraping code (reference)
â”œâ”€â”€ ğŸ“ notebooks/                 # Jupyter notebooks for analysis
â”‚   â””â”€â”€ job_analysis.ipynb        # ğŸ†• Enhanced analysis with location intelligence
â”œâ”€â”€ ğŸ“ data/                      # ğŸ’¾ Database and output files
â”‚   â”œâ”€â”€ jobs.db                   # SQLite database
â”‚   â””â”€â”€ jobs_export.csv           # Latest CSV export
â”œâ”€â”€ ğŸ“„ Makefile                   # ğŸ› ï¸ Build automation with multiple commands
â”œâ”€â”€ ğŸ“„ run_parser.py              # ğŸ¯ Simple parser runner (calls module)
â””â”€â”€ ğŸ“„ pyproject.toml             # Poetry configuration
```

### ğŸ¨ Frontend Architecture

The frontend has been **refactored into a modular structure** for better maintainability:

- **ğŸ¯ Modular Design**: Each tab is a separate module (~80-150 lines vs 1200+ monolithic)
- **ğŸ”§ Reusable Components**: Common UI elements extracted to `components/`
- **ğŸ› ï¸ Shared Utilities**: Database operations and common functions in `utils/`
- **ğŸ“Š Tab-Based Organization**: Live search, stored jobs, AI-enhanced, and history tabs
- **ğŸš€ Developer Friendly**: Easy to add new features or modify existing ones

## ğŸ“Š Enhanced Data Structure

The parser produces **21 columns** of comprehensive job data, including automatic company information extraction:

### ğŸ”§ Core Job Information (Legacy Compatible)
| Column | Description | Example |
|--------|-------------|---------|
| `id` | Unique job identifier (UUID) | `abc123-def4-5678-90ab-cdef12345678` |
| `company` | Company name | `Microsoft` |
| `title` | Job title | `Senior Data Scientist` |
| `level` | Experience level | `Mid-Senior level` |
| `salary_range` | Salary information | `$120,000 - $180,000/year` |
| `content` | Full job description | `We are looking for...` |
| `employment_type` | Employment type | `Full-time` |
| `job_function` | Job category | `Engineering and Information Technology` |
| `industries` | Related industries | `Computer Software` |
| `posted_time` | When posted | `2 days ago` |
| `applicants` | Number of applicants | `47 applicants` |
| `job_id` | LinkedIn's job ID | `3567890123` |
| `date` | Parsing date | `2025-08-21` |
| `parsing_link` | Search URL used | `https://linkedin.com/...` |
| `job_posting_link` | Direct job link | `https://linkedin.com/jobs/...` |

### ğŸ†• Location Intelligence Features
| Column | Description | Example |
|--------|-------------|---------|
| `location` | Extracted location | `San Francisco, CA` |
| `work_location_type` | AI-classified work type | `Remote`, `Hybrid`, `On-site` |

### ï¿½ Company Information (Auto-Extracted & Enhanced Display)
| Column | Description | Example | Frontend Display |
|--------|-------------|---------|------------------|
| `company_size` | Number of employees | `1,000-5,000 employees` | **Company Info column**: ğŸ‘¥ 1,000-5,000 employees |
| `company_followers` | LinkedIn followers | `150,000 followers` | **Company Info column**: ğŸ‘¨â€ğŸ’¼ 150.0K followers |
| `company_industry` | Company industry | `Computer Software` | **Company Info column**: ğŸ­ Computer Software |
| `company_info_link` | LinkedIn company page URL | `https://www.linkedin.com/company/microsoft` | Backend reference |

**âœ¨ Enhanced Frontend Display Example:**
- **Company Column**: "Microsoft"
- **Company Info Column**: "ğŸ­ Technology â€¢ ğŸ‘¥ 10,001+ employees â€¢ ğŸ‘¨â€ğŸ’¼ 29.5M followers"

## ğŸ¢ Company Enrichment Service

The system includes a comprehensive company enrichment service that can be used independently:

### Company Enrichment CLI

```bash
# Show company enrichment statistics
poetry run python -m genai_job_finder.linkedin_parser.company_enrichment --show-missing

# Enrich all companies needing data
poetry run python -m genai_job_finder.linkedin_parser.company_enrichment

# Enrich specific company
poetry run python -m genai_job_finder.linkedin_parser.company_enrichment --company "Microsoft"

# Force re-enrichment of company
poetry run python -m genai_job_finder.linkedin_parser.company_enrichment --company "Microsoft" --force

# Create missing company records
poetry run python -m genai_job_finder.linkedin_parser.company_enrichment --create-missing
```

### Makefile Company Commands

```bash
# Show company statistics
make run-company-enrichment STATS=true

# Enrich companies that need data
make run-company-enrichment ENRICH=true

# Enrich specific company
make run-company-enrichment COMPANY='Microsoft'
```

### Company Service Features

- **ğŸ“Š Statistics Display**: Shows coverage rates and companies needing enrichment
- **ğŸ” Smart Detection**: Identifies companies with missing data automatically  
- **âš¡ Efficient Processing**: Only enriches companies that need additional information
- **ğŸ›¡ï¸ Rate Limiting**: Built-in delays to respect LinkedIn's rate limits
- **ğŸ“ˆ Progress Tracking**: Visual progress bars for bulk operations
- **ğŸ”„ Fallback Handling**: Graceful handling of enrichment failures

## ğŸ¤– Programmatic Usage

### Basic Usage with Company Enrichment
```python
from genai_job_finder.linkedin_parser import LinkedInJobParser, DatabaseManager
from genai_job_finder.linkedin_parser.company_enrichment import CompanyEnrichmentService

# Initialize components with company enrichment
db = DatabaseManager("data/jobs.db")
company_service = CompanyEnrichmentService(database=db)
parser = LinkedInJobParser(database=db, company_service=company_service)

# Parse jobs with integrated company intelligence
jobs = parser.parse_jobs(
    search_query="senior data scientist",
    location="San Francisco",
    total_jobs=100  # Specify number of jobs to collect
)

print(f"Found {len(jobs)} jobs with company enrichment")

# Export to CSV with all enhanced columns including company info
csv_file = db.export_jobs_to_csv("data/my_jobs.csv")
print(f"Exported to: {csv_file}")

# Get company statistics
company_service.show_statistics()

# Get as pandas DataFrame for analysis
df = db.get_jobs_as_dataframe()
print(f"DataFrame shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")
```

### Quick Start Function
```python
# Alternative: Use the built-in runner function
from genai_job_finder.linkedin_parser import run_parser
run_parser()  # Uses default settings: "data scientist" in "San Antonio"
```

## ğŸ¤– AI-Powered Data Cleaning & Enhancement

The system includes a comprehensive AI data cleaner that enhances raw job data with intelligent analysis and structured information extraction.

### ğŸ¯ AI Enhancement Features

- **ğŸ“ Experience Analysis**: Extracts minimum years of experience and classifies into 7 levels
- **ğŸ’° Salary Intelligence**: Parses salary ranges with currency and period normalization  
- **ğŸ  Location Validation**: Validates and corrects work location types (Remote/Hybrid/On-site)
- **ğŸ’¼ Employment Type Standardization**: Validates Full-time/Part-time/Contract/Internship classifications

### ğŸ“Š Experience Level Classification

Jobs are automatically classified into 7 experience levels:

| Level | Years | Label |
|-------|-------|-------|
| 0 | 0 years | Entry level |
| 1 | 1 year | Junior |
| 2 | 2-3 years | Associate/Early career |
| 3 | 4-5 years | Mid-level |
| 4 | 6-8 years | Senior |
| 5 | 9-12 years | Staff/Principal/Lead |
| 6 | 13+ years | Director/VP/Executive |

### ğŸš€ AI Cleaner Usage

#### Enhanced Frontend Pipeline
Use the enhanced frontend for complete parse & clean workflow:

```bash
make run-enhanced-frontend
```

Features a **Parse & Clean** tab that:
- Parses jobs from LinkedIn
- Automatically applies AI enhancement
- Shows real-time progress
- Displays enhanced results immediately

#### Command Line Interface
```bash
# Basic AI cleaning
python -m genai_job_finder.data_cleaner.run_graph

# With custom options
python -m genai_job_finder.data_cleaner.run_graph \
    --db-path data/jobs.db \
    --model llama3.2 \
    --verbose
```

#### Programmatic Usage
```python
from genai_job_finder.data_cleaner import JobDataCleaner

# Initialize AI cleaner
cleaner = JobDataCleaner()

# Clean individual job
enhanced_job = await cleaner.clean_job_data(job_data)

# Process database jobs
cleaner.process_database("data/jobs.db")
```

### ğŸ§ª Individual Component Testing

Test each AI component independently:

```bash
# Test experience extraction
python -m genai_job_finder.data_cleaner.chains.experience_extraction

# Test salary extraction  
python -m genai_job_finder.data_cleaner.chains.salary_extraction

# Test location validation
python -m genai_job_finder.data_cleaner.chains.location_validation

# Test employment validation
python -m genai_job_finder.data_cleaner.chains.employment_validation
```

### ğŸ“ˆ Enhanced Output Fields

The AI cleaner adds these fields to your job data:

#### Experience Enhancement
- `min_years_experience`: Required years (0-15+)
- `experience_level`: Classified level (0-6)
- `experience_level_label`: Human-readable label

#### Salary Enhancement  
- `min_salary`, `max_salary`, `mid_salary`: Salary range breakdown
- `salary_currency`: Currency (USD, EUR, etc.)
- `salary_period`: Period (yearly, monthly, hourly)

#### Validation Enhancement
- `work_location_type_corrected`: Location validation flag
- `employment_type_corrected`: Employment type validation flag

## ğŸ›ï¸ Available Commands

| Command | Description | Usage |
|---------|-------------|-------|
| `make run-parser` | ğŸ¯ Run LinkedIn parser with company intelligence | **Recommended** |
| `make run-parser-mod` | ğŸ”§ Run LinkedIn parser as module | Advanced usage |
| `make run-company-enrichment` | ğŸ¢ Run company enrichment pipeline separately | **Company data management** |
| `make run-pipeline` | ğŸš€ Run parser + company enrichment + AI cleaner | **Full processing** |
| `make run-cleaner` | ğŸ¤– Run AI data cleaner only | Process existing data |
| `make run-frontend` | ğŸ–¥ï¸ Launch enhanced Streamlit web app | **Interactive UI with Company Info** |
| `make install` | ğŸ“¦ Install dependencies | First-time setup |
| `make test` | ğŸ§ª Run tests | Development |
| `make clean` | ğŸ§¹ Clean temporary files | Maintenance |
| `make help` | â“ Show all available commands | Get help |

## ğŸ”§ Advanced Configuration

### Location Intelligence Features

The parser automatically provides:

- **ğŸ¯ Smart Location Extraction**: Parses job locations from various formats
- **ğŸ¤– AI-Powered Classification**: Intelligently categorizes work arrangements:
  - **ğŸ  Remote**: Jobs with remote work keywords (`remote`, `work from home`, `distributed`)
  - **ğŸ”„ Hybrid**: Jobs mentioning flexible arrangements (`hybrid`, `flexible`, `remote optional`)
  - **ğŸ¢ On-site**: Traditional office-based positions

### Customizing Search Parameters

```python
# Advanced search customization
jobs = parser.parse_jobs(
    search_query="machine learning engineer",  # Job keywords
    location="New York",                       # Location filter
    total_jobs=200,                           # Number of jobs to collect
    time_filter="r604800",                    # Last 7 days (default: r86400 = 24h)
    remote=False,                             # Include remote jobs
    parttime=False                            # Include part-time jobs
)
```

### Time Filters

The enhanced time filtering system supports:
- **â° Past hour** (`r3600`): Most recent job postings
- **ğŸ“… Past 24 hours** (`r86400`): Default filter 
- **ğŸ“† Past week** (`r604800`): Weekly job updates
- **ï¿½ Past month** (`r2592000`): Monthly comprehensive search

*Note: The time filter bug has been fixed - selections now properly filter LinkedIn API calls instead of defaulting to 24 hours.*

## ğŸ“ˆ Recent Major Updates

### ï¿½ Company Enrichment & Frontend Enhancement (v4.0)
- âœ… **Separate Company Info Column** - Dedicated "Company Info" column with rich metadata display
- âœ… **Company Enrichment Service** - Independent pipeline with lookup-first optimization
- âœ… **Performance Optimization** - 3-5x faster processing for existing companies
- âœ… **Enhanced Frontend Display** - Company info formatted as "ğŸ­ Industry â€¢ ğŸ‘¥ Size â€¢ ğŸ‘¨â€ğŸ’¼ Followers"
- âœ… **Database Architecture** - Separate companies table with foreign key relationships
- âœ… **Smart Followers Formatting** - Automatic conversion to K/M format (e.g., "467.3K followers")

### ï¿½ğŸ¯ Frontend Refactoring & Time Filter Fix (v3.0)
- âœ… **Modular frontend architecture** - Split 1200+ line monolith into organized modules
- âœ… **Enhanced time filtering** - Added "Past hour" option and fixed hardcoded filter bug
- âœ… **Improved developer experience** - Each tab in separate file for better maintainability
- âœ… **Streamlined Makefile** - Single `run-frontend` command with all features integrated
- âœ… **Clean project structure** - Removed unnecessary shell scripts, organized utilities

### ğŸ¤– AI Data Cleaning Integration (v2.5)
- âœ… **Complete AI pipeline** - Automatic job enhancement with experience, salary, and location analysis
- âœ… **Real-time processing** - Live search results enhanced with AI in frontend
- âœ… **Comprehensive enhancement** - 7-level experience classification, salary extraction, location validation
- âœ… **Visual progress tracking** - Real-time AI processing status and statistics

### ğŸ¯ LinkedIn Parser Enhancement (v2.0)
- âœ… **Complete architecture rewrite** with modular structure
- âœ… **17-column data structure** matching legacy format exactly
- âœ… **Location intelligence engine** with automatic extraction and classification
- âœ… **Multiple execution methods** (script, module, programmatic)
- âœ… **Enhanced database schema** with automatic migration support
- âœ… **Improved error handling** and rate limiting
- âœ… **Comprehensive documentation** and examples

### ğŸ†• Key Features Added
- **ï¿½ Separate Company Enrichment**: Independent company intelligence pipeline with lookup-first optimization
- **ğŸ“‹ Company Info Column**: Dedicated frontend column for enhanced company metadata display  
- **ï¿½ğŸŒ Location Intelligence**: Automatic location extraction and work type classification
- **ğŸ”§ Modular Architecture**: Proper Python package structure with company service separation
- **ğŸ“Š Enhanced Analytics**: Updated analytics with company intelligence insights
- **âš¡ Multiple Entry Points**: Run as script, module, or import programmatically
- **ğŸ’¾ Smart Data Export**: All outputs include comprehensive company information
- **ğŸ›ï¸ Comprehensive CLI**: Multiple Makefile commands including company enrichment

### ğŸ”„ Migration & Compatibility
- **âœ… Full backward compatibility** with existing data and workflows
- **âœ… Automatic database migration** when running updated parser with companies table
- **âœ… Legacy format preserved** - no breaking changes to output structure
- **âœ… Enhanced with new fields** - company enrichment and display improvements

## ğŸ” Example Outputs

### Command Line
```bash
$ make run-parser
Starting LinkedIn job parsing...
Getting job IDs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.60s/it]
Getting job details: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:45<00:00,  2.28s/it]
âœ… Successfully parsed 20 jobs
ğŸ“Š Jobs exported to: data/jobs_export.csv
```

### CSV Output Sample
```csv
id,company,title,location,work_location_type,level,salary_range,company_size,company_followers,company_industry...
abc123...,Microsoft,Senior Data Scientist,Seattle WA,Hybrid,Mid-Senior level,$150k-200k,10001+ employees,29500000 followers,Technology...
def456...,Google,ML Engineer,San Francisco CA,Remote,Senior level,$180k-250k,10001+ employees,30000000 followers,Technology...
```

### Frontend Company Info Display
```
Company          | Company Info                                      | Title                | Location
Microsoft        | ğŸ­ Technology â€¢ ğŸ‘¥ 10,001+ employees â€¢ ğŸ‘¨â€ğŸ’¼ 29.5M   | Senior Data Scientist | Seattle, WA
Google           | ğŸ­ Technology â€¢ ğŸ‘¥ 10,001+ employees â€¢ ğŸ‘¨â€ğŸ’¼ 30.0M   | ML Engineer          | San Francisco, CA
The Swift Group | ğŸ­ IT Services â€¢ ğŸ‘¥ 51-200 employees â€¢ ğŸ‘¨â€ğŸ’¼ 13.6K   | DevOps Engineer      | San Antonio, TX
```

### Location Intelligence Results
- **ğŸ¢ On-site**: 42% of jobs (traditional office-based)
- **ğŸ  Remote**: 24% of jobs (work from anywhere)  
- **ğŸ”„ Hybrid**: 24% of jobs (flexible arrangements)
- **ğŸ“Š High accuracy**: 90%+ location data coverage

## ğŸ›¡ï¸ Best Practices

- **Rate limiting**: Built-in delays between requests (2-3 seconds)
- **Respectful scraping**: User-agent rotation and proper headers
- **Error handling**: Comprehensive retry logic and graceful failures
- **Data integrity**: Automatic duplicate prevention and validation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes with tests
4. Submit a pull request

## ğŸ› Troubleshooting

### Common Issues

1. **Import Errors**: Ensure Poetry environment is activated
   ```bash
   poetry shell
   ```

2. **No Jobs Found**: Check internet connection and search parameters

3. **Database Issues**: Database auto-migrates, but you can reset:
   ```bash
   rm data/jobs.db  # Reset database if needed
   ```

### Getting Help

- Use `make help` to see all available commands
- Check `notebooks/` for data analysis examples
- Review `run_parser.py` for usage patterns

## ğŸ“ License

This project is for educational and personal use. Please respect LinkedIn's Terms of Service.

---

**ğŸ¯ Ready to start? Just run `make run-parser` and begin collecting job data with enhanced location intelligence!**
3. **View parsing history** - See recent parsing runs

### Example Custom Search

```python
from genai_job_finder import LinkedInJobParser, DatabaseManager

# Initialize database and parser
db = DatabaseManager("data/jobs.db")
parser = LinkedInJobParser(database=db)

# Parse jobs
jobs = parser.parse_jobs(
    search_query="senior data scientist",
    location="San Francisco",
    max_pages=3
)

print(f"Found {len(jobs)} jobs")
```

### Analyzing Job Data

Open the Jupyter notebook for detailed job analysis:

```bash
jupyter notebook notebooks/job_analysis.ipynb
```

The notebook provides:
- Total job counts and statistics
- Top 10 most recent jobs with details
- Company and location distribution
- Salary information analysis
- Remote work statistics

## ğŸ“Š Data Models

### Job Model
```python
@dataclass
class Job:
    job_id: str
    title: str
    company: str
    location: str
    description: str
    posted_date: Optional[datetime]
    salary_range: Optional[str]
    job_type: Optional[JobType]
    experience_level: Optional[ExperienceLevel]
    remote_option: bool
    easy_apply: bool
    linkedin_url: Optional[str]
    # ... additional fields
```

### JobRun Model
```python
@dataclass
class JobRun:
    id: Optional[int]
    run_date: datetime
    search_query: str
    location_filter: str
    job_count: int
    status: str
    # ... additional tracking fields
```

## ğŸ”§ Configuration

### Search Parameters

Configure search parameters in `genai_job_finder/legacy/config.py`:

```python
LINKEDIN_JOB_SEARCH_PARAMS = [
    {
        "keywords": "senior data scientist",
        "location": "San Antonio",
        "f_TPR": "r86400",  # last 24 hours
        "remote": False,
    },
    # Add more search configurations...
]
```

### Time Filters
- `r86400`: Last 24 hours
- `r604800`: Last 7 days  
- `r2592000`: Last 30 days

## ğŸ¤– AI Integration

The project includes AI frameworks for future enhancements:

- **LangChain**: For building AI applications
- **LangGraph**: For complex AI workflows
- **OpenAI Integration**: GPT models support
- **Ollama Support**: Local LLM integration
- **Chroma Vector Store**: Semantic job search

## ğŸ“ˆ Database Schema

### Tables

1. **jobs** - Stores individual job listings
2. **job_runs** - Tracks parsing sessions

### Key Features
- Automatic duplicate prevention
- Run tracking and status monitoring
- Comprehensive job metadata storage
- SQLite for lightweight, portable database

## ğŸš¦ Getting Started

1. **First Run:**
```bash
poetry run python example_usage.py
```
Choose option 1 to parse jobs using default configuration.

2. **View Results:**
Open `notebooks/job_analysis.ipynb` and run all cells to see your collected job data.

3. **Customize:**
- Modify search parameters in the config file
- Add new search terms and locations
- Adjust the number of pages to parse

## ğŸ” Example Output

```
Total jobs in database: 212
Recent parsing runs:
- Run 7: senior data scientist (21 jobs) - completed
- Run 6: senior data scientist (12 jobs) - completed

Top Companies:
â€¢ EY: 3 jobs
â€¢ Lensa: 2 jobs
â€¢ USAA: 2 jobs

Remote Jobs: 45% of listings
Easy Apply: 67% of listings
```

## ğŸ›¡ï¸ Best Practices & Rate Limiting

- **â±ï¸ Built-in delays**: 2-3 seconds between requests
- **ğŸ¤– Respectful scraping**: Proper user-agent headers and request patterns
- **ğŸ”„ Error handling**: Comprehensive retry logic and graceful failure handling
- **ğŸ“Š Progress tracking**: Visual progress bars for long-running operations
- **ğŸ’¾ Data integrity**: Automatic duplicate prevention and data validation

## ğŸ§ª Data Analysis

The enhanced Jupyter notebook (`notebooks/job_analysis.ipynb`) provides:

- **ğŸ“Š Location intelligence analytics** with work type distribution
- **ğŸ¢ Company and location insights** with visual statistics
- **ğŸ’° Salary analysis** by work type and location
- **ğŸ“ˆ Trending job functions** and industries
- **ğŸ¯ Data quality metrics** and validation reports

**To use:**
```bash
jupyter notebook notebooks/job_analysis.ipynb
```

## ğŸ¤ Contributing

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature-name`
3. **Make** your changes with proper tests
4. **Ensure** all commands work: `make test`
5. **Submit** a pull request with detailed description

## ğŸ› Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| **Import Errors** | Ensure Poetry environment: `poetry shell` |
| **No Jobs Found** | Check search parameters and internet connection |
| **Database Locked** | Close any open database connections |
| **Permission Errors** | Ensure write access to `data/` folder |

### Getting Help

- ğŸ“– **Documentation**: This README covers all features
- ğŸ’» **Examples**: Check `run_parser.py` and notebook examples
- ğŸ”§ **Commands**: Run `make help` for all available commands
- ğŸ§ª **Testing**: Use `make test` to validate installation

## ğŸ“Š Performance Metrics

- **ğŸ¯ Success Rate**: 95%+ job collection reliability
- **âš¡ Performance**: ~10-20 jobs per page, 2-3 seconds per request
- **ğŸ–ï¸ Data Quality**: 90%+ location intelligence coverage
- **ğŸ’¾ Storage**: Efficient SQLite database with migration support
- **ğŸ“ˆ Scalability**: Handles hundreds of jobs with progress tracking

---

## ğŸ‰ Why Choose GenAI Job Finder?

### âœ… **Single Command Solution**
- **Before**: Multiple commands for parsing, company data, manual enhancement, etc.
- **After**: One `make run-parser` command with integrated company intelligence!

### âœ… **Dedicated Company Info Display** 
- **Separate Company Info Column**: Rich metadata display with emojis and formatting
- **Clean Organization**: Company name and details properly separated
- **Consistent Display**: Works across all frontend tabs and exports
- **Smart Formatting**: Automatic number formatting (29.5M followers, 13.6K followers)

### âœ… **Built-in Company Intelligence** 
- **60-70% coverage** for company size and followers with smart enrichment service
- **3-5x performance improvement** through lookup-first optimization
- **Smart rate limiting** prevents LinkedIn blocks
- **Independent pipeline** for company data management

### âœ… **Production Ready**
- **Enhanced output** with comprehensive job + company data
- **Separate companies table** with foreign key relationships
- **Built-in error handling** and recovery with company service fallbacks
- **Progress tracking** with detailed company enrichment statistics
- **Automatic CSV export** with full company information

### âœ… **Developer Friendly**
- **Modular architecture** for easy customization
- **Comprehensive documentation** with examples
- **Streamlit web interface** for interactive use
- **AI integration** for data enhancement

---

## ğŸ“ License & Usage

This project is designed for **educational and personal use**. Please use responsibly and in accordance with LinkedIn's Terms of Service.

**ğŸš€ Ready to start? Run `make run-parser` and collect comprehensive job data with company intelligence in one command!**
