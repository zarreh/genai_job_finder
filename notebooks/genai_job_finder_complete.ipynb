{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI Job Finder - Resume to Job Matching System\n",
    "\n",
    "This notebook implements a comprehensive job matching system that:\n",
    "\n",
    "1. Loads job data from SQLite database\n",
    "2. Processes resume files (PDF/DOC)\n",
    "3. Uses LangChain + Ollama Llama3.2 for semantic matching\n",
    "4. Incorporates user preferences\n",
    "5. Returns ranked job recommendations with explanations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langchain langchain-community langchain-ollama\n",
    "# !pip install pypdf2 python-docx pandas sqlite3 numpy scikit-learn\n",
    "# !pip install sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/.cache/pypoetry/virtualenvs/genai-job-finder-Y_k-9c-5-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alireza/.cache/pypoetry/virtualenvs/genai-job-finder-Y_k-9c-5-py3.12/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/alireza/.cache/pypoetry/virtualenvs/genai-job-finder-Y_k-9c-5-py3.12/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Document processing\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "\n",
    "# ML and similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document as LangChainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "print(\"✅ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Loading (Your Provided Code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported custom modules\n",
      "Database path: /home/alireza/projects/genai_job_finder/data/jobs.db\n",
      "Database exists: True\n",
      "✅ Database found\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path\n",
    "project_root = (\n",
    "    Path(__file__).parent.parent if \"__file__\" in globals() else Path.cwd().parent\n",
    ")\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "try:\n",
    "    from genai_job_finder.linkedin_parser.database import DatabaseManager\n",
    "    from genai_job_finder.linkedin_parser.models import Job, JobRun\n",
    "\n",
    "    print(\"✅ Successfully imported custom modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️  Could not import custom modules: {e}\")\n",
    "    print(\"Will use direct SQLite queries instead\")\n",
    "\n",
    "# Initialize database connection\n",
    "db_path = project_root / \"data\" / \"jobs.db\"\n",
    "# Alternative path for testing\n",
    "# db_path = project_root / \"test_jobs.db\"\n",
    "\n",
    "print(f\"Database path: {db_path}\")\n",
    "print(f\"Database exists: {db_path.exists()}\")\n",
    "\n",
    "if not db_path.exists():\n",
    "    print(\"❌ Database not found. Please check the path.\")\n",
    "else:\n",
    "    print(\"✅ Database found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load job data from database\n",
    "def load_jobs_data(db_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load jobs data from SQLite database\"\"\"\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        # Get the latest job_run created_at timestamp\n",
    "        latest_run_query = \"SELECT MAX(created_at) as latest_run FROM job_runs WHERE status = 'completed'\"\n",
    "        latest_run = pd.read_sql_query(latest_run_query, conn).iloc[0][\"latest_run\"]\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            company,\n",
    "            company_size,\n",
    "            company_followers,\n",
    "            company_industry,\n",
    "            title,\n",
    "            location,\n",
    "            work_location_type,\n",
    "            level,\n",
    "            salary_range,\n",
    "            employment_type,\n",
    "            job_function,\n",
    "            industries,\n",
    "            posted_time,\n",
    "            applicants,\n",
    "            job_id,\n",
    "            date,\n",
    "            parsing_link,\n",
    "            job_posting_link,\n",
    "            company_info_link,\n",
    "            created_at\n",
    "        FROM jobs \n",
    "        WHERE created_at > '{latest_run}'\n",
    "        ORDER BY created_at DESC    \n",
    "        \"\"\"\n",
    "\n",
    "        return pd.read_sql_query(query, conn)\n",
    "\n",
    "\n",
    "# Load the data\n",
    "if db_path.exists():\n",
    "    jobs_df = load_jobs_data(db_path)\n",
    "    # print(f\"\\n📋 Sample of loaded data:\")\n",
    "    # print(jobs_df[[\"company\", \"title\", \"location\", \"level\", \"job_function\"]].head())\n",
    "else:\n",
    "    print(\"❌ Cannot load data - database not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resume Processing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeProcessor:\n",
    "    \"\"\"Process resume files and extract relevant information\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=200\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_text_from_docx(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from DOCX file\"\"\"\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DOCX: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def load_resume(self, file_path: str) -> str:\n",
    "        \"\"\"Load resume from file (PDF or DOCX)\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Resume file not found: {file_path}\")\n",
    "\n",
    "        if file_path.suffix.lower() == \".pdf\":\n",
    "            return self.extract_text_from_pdf(str(file_path))\n",
    "        elif file_path.suffix.lower() in [\".docx\", \".doc\"]:\n",
    "            return self.extract_text_from_docx(str(file_path))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n",
    "\n",
    "    def extract_resume_features(self, resume_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract structured features from resume using LLM\"\"\"\n",
    "\n",
    "        extraction_prompt = PromptTemplate(\n",
    "            input_variables=[\"resume_text\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the following resume and extract key information in JSON format.\n",
    "            \n",
    "            Resume Text:\n",
    "            {resume_text}\n",
    "            \n",
    "            Please extract and return ONLY a valid JSON object with the following structure:\n",
    "            {{\n",
    "                \"skills\": [\"list of technical and soft skills\"],\n",
    "                \"experience_years\": \"estimated years of experience as integer\",\n",
    "                \"job_titles\": [\"previous job titles\"],\n",
    "                \"industries\": [\"industries worked in\"],\n",
    "                \"education\": [\"degrees and certifications\"],\n",
    "                \"key_achievements\": [\"notable achievements\"],\n",
    "                \"preferred_roles\": [\"types of roles this person seems suited for\"]\n",
    "            }}\n",
    "            \n",
    "            Return only the JSON object, no additional text.\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            chain = extraction_prompt | self.llm\n",
    "            response = chain.invoke(\n",
    "                {\"resume_text\": resume_text[:4000]}\n",
    "            )  # Limit text length\n",
    "\n",
    "            # Clean and parse JSON response\n",
    "            json_str = response.strip()\n",
    "            if json_str.startswith(\"```json\"):\n",
    "                json_str = json_str[7:-3]\n",
    "            elif json_str.startswith(\"```\"):\n",
    "                json_str = json_str[3:-3]\n",
    "\n",
    "            features = json.loads(json_str)\n",
    "            return features\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {e}\")\n",
    "            # Return basic fallback structure\n",
    "            return {\n",
    "                \"skills\": [],\n",
    "                \"experience_years\": 0,\n",
    "                \"job_titles\": [],\n",
    "                \"industries\": [],\n",
    "                \"education\": [],\n",
    "                \"key_achievements\": [],\n",
    "                \"preferred_roles\": [],\n",
    "            }\n",
    "\n",
    "\n",
    "# print(\"✅ Resume processor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobMatcher:\n",
    "    \"\"\"Main job matching engine using multiple similarity metrics\"\"\"\n",
    "\n",
    "    def __init__(self, llm, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.llm = llm\n",
    "        self.use_sentence_transformer = False\n",
    "        self.sentence_model = None\n",
    "\n",
    "        # Check if sentence transformers are available from the setup\n",
    "        if globals().get(\"SENTENCE_TRANSFORMER_AVAILABLE\", False):\n",
    "            try:\n",
    "                import torch\n",
    "\n",
    "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "                # Try to initialize sentence transformer with fallback models\n",
    "                model_names = [\n",
    "                    embedding_model_name,\n",
    "                    \"paraphrase-MiniLM-L3-v2\",\n",
    "                    \"all-MiniLM-L6-v2\",\n",
    "                ]\n",
    "\n",
    "                for model_name in model_names:\n",
    "                    try:\n",
    "                        self.sentence_model = SentenceTransformer(\n",
    "                            model_name, device=device, cache_folder=\"./models\"\n",
    "                        )\n",
    "                        self.use_sentence_transformer = True\n",
    "                        print(f\"✅ Using sentence transformer: {model_name}\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Failed to load {model_name}: {e}\")\n",
    "                        continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Sentence transformer initialization failed: {e}\")\n",
    "                self.use_sentence_transformer = False\n",
    "\n",
    "        if not self.use_sentence_transformer:\n",
    "            print(\"📝 Falling back to enhanced TF-IDF similarity\")\n",
    "\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=1000, stop_words=\"english\", ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "    def prepare_job_texts(self, jobs_df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Prepare job descriptions for matching\"\"\"\n",
    "        job_texts = []\n",
    "\n",
    "        for _, job in jobs_df.iterrows():\n",
    "            # Combine relevant job information\n",
    "            text_parts = [\n",
    "                str(job.get(\"title\", \"\")),\n",
    "                str(job.get(\"company\", \"\")),\n",
    "                str(job.get(\"job_function\", \"\")),\n",
    "                str(job.get(\"industries\", \"\")),\n",
    "                str(job.get(\"level\", \"\")),\n",
    "                str(job.get(\"location\", \"\")),\n",
    "            ]\n",
    "\n",
    "            job_text = \" \".join([part for part in text_parts if part and part != \"nan\"])\n",
    "            job_texts.append(job_text)\n",
    "\n",
    "        return job_texts\n",
    "\n",
    "    def calculate_semantic_similarity(\n",
    "        self, resume_text: str, job_texts: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Calculate semantic similarity using sentence transformers or fallback to enhanced TF-IDF\"\"\"\n",
    "        if self.use_sentence_transformer and self.sentence_model:\n",
    "            try:\n",
    "                # Create embeddings\n",
    "                resume_embedding = self.sentence_model.encode([resume_text])\n",
    "                job_embeddings = self.sentence_model.encode(job_texts)\n",
    "\n",
    "                # Calculate cosine similarity\n",
    "                similarities = cosine_similarity(resume_embedding, job_embeddings)[0]\n",
    "                return similarities\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Sentence transformer failed, using TF-IDF: {e}\")\n",
    "                self.use_sentence_transformer = False\n",
    "\n",
    "        # Fallback to enhanced TF-IDF\n",
    "        return self.calculate_enhanced_tfidf_similarity(resume_text, job_texts)\n",
    "\n",
    "    def calculate_enhanced_tfidf_similarity(\n",
    "        self, resume_text: str, job_texts: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Enhanced TF-IDF similarity with preprocessing\"\"\"\n",
    "\n",
    "        # Preprocess texts\n",
    "        def preprocess_text(text):\n",
    "            # Convert to lowercase and remove extra spaces\n",
    "            text = re.sub(r\"\\s+\", \" \", text.lower().strip())\n",
    "            return text\n",
    "\n",
    "        resume_text = preprocess_text(resume_text)\n",
    "        job_texts = [preprocess_text(text) for text in job_texts]\n",
    "\n",
    "        # Use enhanced TF-IDF\n",
    "        enhanced_vectorizer = TfidfVectorizer(\n",
    "            max_features=2000,\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1, 3),  # Include trigrams for better matching\n",
    "            min_df=1,\n",
    "            max_df=0.95,\n",
    "        )\n",
    "\n",
    "        all_texts = [resume_text] + job_texts\n",
    "        tfidf_matrix = enhanced_vectorizer.fit_transform(all_texts)\n",
    "\n",
    "        # Calculate similarity between resume and jobs\n",
    "        resume_vector = tfidf_matrix[0:1]\n",
    "        job_vectors = tfidf_matrix[1:]\n",
    "\n",
    "        similarities = cosine_similarity(resume_vector, job_vectors)[0]\n",
    "        return similarities\n",
    "\n",
    "    def calculate_keyword_similarity(\n",
    "        self, resume_text: str, job_texts: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Calculate TF-IDF based keyword similarity\"\"\"\n",
    "        # Fit TF-IDF on all texts\n",
    "        all_texts = [resume_text] + job_texts\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)\n",
    "\n",
    "        # Calculate similarity between resume and jobs\n",
    "        resume_vector = tfidf_matrix[0:1]\n",
    "        job_vectors = tfidf_matrix[1:]\n",
    "\n",
    "        similarities = cosine_similarity(resume_vector, job_vectors)[0]\n",
    "        return similarities\n",
    "\n",
    "    def calculate_preference_score(\n",
    "        self, job_row: pd.Series, user_preferences: Dict[str, Any]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate how well job matches user preferences\"\"\"\n",
    "        score = 0.0\n",
    "        max_score = 0.0\n",
    "\n",
    "        # Location preference\n",
    "        if \"preferred_locations\" in user_preferences:\n",
    "            max_score += 1.0\n",
    "            job_location = str(job_row.get(\"location\", \"\")).lower()\n",
    "            work_location = str(job_row.get(\"work_location\", \"\")).lower()\n",
    "\n",
    "            for pref_loc in user_preferences[\"preferred_locations\"]:\n",
    "                if (\n",
    "                    pref_loc.lower() in job_location\n",
    "                    or pref_loc.lower() in work_location\n",
    "                ):\n",
    "                    score += 1.0\n",
    "                    break\n",
    "\n",
    "        # Salary preference\n",
    "        if \"salary_range\" in user_preferences:\n",
    "            max_score += 1.0\n",
    "            salary_range = str(job_row.get(\"salary_range\", \"\"))\n",
    "            if salary_range and salary_range != \"nan\":\n",
    "                # Simple salary matching logic\n",
    "                score += 0.5  # Partial credit for having salary info\n",
    "\n",
    "        # Job function preference\n",
    "        if \"job_functions\" in user_preferences:\n",
    "            max_score += 1.0\n",
    "            job_function = str(job_row.get(\"job_function\", \"\")).lower()\n",
    "\n",
    "            for pref_func in user_preferences[\"job_functions\"]:\n",
    "                if pref_func.lower() in job_function:\n",
    "                    score += 1.0\n",
    "                    break\n",
    "\n",
    "        # Employment type preference\n",
    "        if \"employment_type\" in user_preferences:\n",
    "            max_score += 1.0\n",
    "            emp_type = str(job_row.get(\"employment_type\", \"\")).lower()\n",
    "\n",
    "            if user_preferences[\"employment_type\"].lower() in emp_type:\n",
    "                score += 1.0\n",
    "\n",
    "        return score / max_score if max_score > 0 else 0.0\n",
    "\n",
    "    def match_jobs(\n",
    "        self,\n",
    "        resume_features: Dict[str, Any],\n",
    "        resume_text: str,\n",
    "        jobs_df: pd.DataFrame,\n",
    "        user_preferences: Dict[str, Any],\n",
    "        top_k: int = 10,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Main matching function that combines all similarity metrics\"\"\"\n",
    "\n",
    "        print(\"🔍 Starting job matching process...\")\n",
    "\n",
    "        # Prepare job texts\n",
    "        job_texts = self.prepare_job_texts(jobs_df)\n",
    "        print(f\"📝 Prepared {len(job_texts)} job descriptions\")\n",
    "\n",
    "        # Calculate different similarity scores\n",
    "        if self.use_sentence_transformer:\n",
    "            print(\"🧠 Calculating semantic similarities with sentence transformer...\")\n",
    "        else:\n",
    "            print(\"🧠 Calculating semantic similarities with enhanced TF-IDF...\")\n",
    "        semantic_scores = self.calculate_semantic_similarity(resume_text, job_texts)\n",
    "\n",
    "        print(\"🔤 Calculating keyword similarities...\")\n",
    "        keyword_scores = self.calculate_keyword_similarity(resume_text, job_texts)\n",
    "\n",
    "        print(\"⚙️ Calculating preference scores...\")\n",
    "        preference_scores = []\n",
    "        for _, job_row in jobs_df.iterrows():\n",
    "            pref_score = self.calculate_preference_score(job_row, user_preferences)\n",
    "            preference_scores.append(pref_score)\n",
    "        preference_scores = np.array(preference_scores)\n",
    "\n",
    "        # Adjust weights based on available methods\n",
    "        if self.use_sentence_transformer:\n",
    "            weights = {\"semantic\": 0.4, \"keyword\": 0.3, \"preference\": 0.3}\n",
    "        else:\n",
    "            # Give more weight to keyword matching when no sentence transformer\n",
    "            weights = {\n",
    "                \"semantic\": 0.5,  # Enhanced TF-IDF\n",
    "                \"keyword\": 0.2,\n",
    "                \"preference\": 0.3,\n",
    "            }\n",
    "\n",
    "        final_scores = (\n",
    "            weights[\"semantic\"] * semantic_scores\n",
    "            + weights[\"keyword\"] * keyword_scores\n",
    "            + weights[\"preference\"] * preference_scores\n",
    "        )\n",
    "\n",
    "        # Add scores to dataframe\n",
    "        jobs_with_scores = jobs_df.copy()\n",
    "        jobs_with_scores[\"semantic_score\"] = semantic_scores\n",
    "        jobs_with_scores[\"keyword_score\"] = keyword_scores\n",
    "        jobs_with_scores[\"preference_score\"] = preference_scores\n",
    "        jobs_with_scores[\"final_score\"] = final_scores\n",
    "\n",
    "        # Sort by final score and return top matches\n",
    "        top_matches = jobs_with_scores.nlargest(top_k, \"final_score\")\n",
    "\n",
    "        method = (\n",
    "            \"sentence transformer\"\n",
    "            if self.use_sentence_transformer\n",
    "            else \"enhanced TF-IDF\"\n",
    "        )\n",
    "        print(f\"✅ Found top {len(top_matches)} job matches using {method}\")\n",
    "        return top_matches\n",
    "\n",
    "\n",
    "# print(\"✅ Job matcher class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration and User Inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing Ollama LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 22:01:24,133 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM initialized successfully. Test response: OK...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama LLM\n",
    "print(\"🚀 Initializing Ollama LLM...\")\n",
    "try:\n",
    "    llm = OllamaLLM(model=\"llama3.2\", temperature=0.1)\n",
    "    # Test the connection\n",
    "    test_response = llm.invoke(\"Hello, respond with 'OK' if you can hear me.\")\n",
    "    print(f\"✅ LLM initialized successfully. Test response: {test_response[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing LLM: {e}\")\n",
    "    print(\"Make sure Ollama is running and llama3.2 model is installed\")\n",
    "    print(\"Run: ollama pull llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Configuration set:\n",
      "   Resume file: /home/alireza/projects/genai_job_finder/genai_job_finder/data/Ali Zarreh_CV_2025_08_30.docx\n",
      "   Preferred locations: ['San Antonio, TX', 'Remote']\n",
      "   Job functions: []\n",
      "   Top matches to return: 15\n"
     ]
    }
   ],
   "source": [
    "# 📁 RESUME FILE PATH - UPDATE THIS PATH\n",
    "RESUME_FILE_PATH = \"/home/alireza/projects/genai_job_finder/genai_job_finder/data/Ali Zarreh_CV_2025_08_30.docx\"  # Change this to your resume file\n",
    "\n",
    "# 👤 USER PREFERENCES - CUSTOMIZE THESE\n",
    "USER_PREFERENCES = {\n",
    "    \"preferred_locations\": [\"San Antonio, TX\", \"Remote\"],\n",
    "    \"salary_range\": {\"min\": 150000, \"max\": 350000},\n",
    "    \"job_functions\": [],  # [\"Information Technology\", \"Engineering\", \"Data Science\"],\n",
    "    \"employment_type\": \"Full-time\",\n",
    "    \"level_preference\": [\"Mid-Senior level\", \"Senior level\"],\n",
    "    \"must_have_keywords\": [],  # [\"Python\", \"Data\", \"Analytics\"],\n",
    "    \"exclude_keywords\": [],  # [\"Sales\", \"Marketing\"],\n",
    "}\n",
    "\n",
    "# 🎯 MATCHING PARAMETERS\n",
    "TOP_K_MATCHES = 15  # Number of top matches to return\n",
    "\n",
    "print(\"⚙️ Configuration set:\")\n",
    "print(f\"   Resume file: {RESUME_FILE_PATH}\")\n",
    "print(f\"   Preferred locations: {USER_PREFERENCES['preferred_locations']}\")\n",
    "print(f\"   Job functions: {USER_PREFERENCES['job_functions']}\")\n",
    "print(f\"   Top matches to return: {TOP_K_MATCHES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing processors...\n",
      "📝 Falling back to enhanced TF-IDF similarity\n",
      "✅ Processors initialized\n",
      "\n",
      "📄 STEP 1: Processing Resume\n",
      "========================================\n",
      "✅ Resume loaded successfully (9802 characters)\n",
      "🧠 Extracting resume features using LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 22:01:24,502 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resume features extracted:\n",
      "   skills: 4 items - ['Advanced expertise in Python, R, SQL, Scala', 'Proficient in employing deep learning frameworks like TensorFlow, Keras, PyTorch', 'Skilled in machine learning algorithms via Scikit-learn, XGBoost, Spark MLlib']...\n",
      "   experience_years: 3\n",
      "   job_titles: 1 items - ['Senior Data Scientist']\n",
      "   industries: 2 items - ['Retail', 'Manufacturing']\n",
      "   education: 3 items - ['Ph.D. Mechanical and Manufacturing Systems Engineering (Cybersecurity)', 'Master of Science in Mechanical Engineering', 'Bachelor of Science in Mechanical Engineering']\n",
      "   key_achievements: 2 items - ['Automated credit-card expense classification', 'Built an agentic RAG documentation assistant']\n",
      "   preferred_roles: 3 items - ['Data Scientist', 'Lead Data Scientist', 'Director of Data Science']\n"
     ]
    }
   ],
   "source": [
    "# Initialize processors\n",
    "print(\"🔧 Initializing processors...\")\n",
    "resume_processor = ResumeProcessor(llm)\n",
    "job_matcher = JobMatcher(llm)\n",
    "print(\"✅ Processors initialized\")\n",
    "\n",
    "# Step 1: Load and process resume\n",
    "print(\"\\n📄 STEP 1: Processing Resume\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # For demo purposes, create a sample resume text if file doesn't exist\n",
    "    if not Path(RESUME_FILE_PATH).exists():\n",
    "        print(f\"⚠️  Resume file not found at {RESUME_FILE_PATH}\")\n",
    "        print(\"Using sample resume for demonstration...\")\n",
    "\n",
    "        resume_text = \"\"\"\n",
    "        John Doe\n",
    "        Senior Data Scientist\n",
    "        \n",
    "        EXPERIENCE:\n",
    "        - 5+ years in data science and machine learning\n",
    "        - Expert in Python, SQL, and statistical analysis\n",
    "        - Experience with cloud platforms (AWS, Azure)\n",
    "        - Led data analytics projects for Fortune 500 companies\n",
    "        \n",
    "        SKILLS:\n",
    "        - Programming: Python, R, SQL, JavaScript\n",
    "        - Machine Learning: scikit-learn, TensorFlow, PyTorch\n",
    "        - Data Visualization: Tableau, Power BI, matplotlib\n",
    "        - Cloud: AWS, Azure, Google Cloud Platform\n",
    "        - Databases: PostgreSQL, MongoDB, Redis\n",
    "        \n",
    "        EDUCATION:\n",
    "        - M.S. in Data Science, University of Texas\n",
    "        - B.S. in Computer Science, Texas A&M University\n",
    "        \n",
    "        CERTIFICATIONS:\n",
    "        - AWS Certified Solutions Architect\n",
    "        - Google Cloud Professional Data Engineer\n",
    "        \"\"\"\n",
    "    else:\n",
    "        resume_text = resume_processor.load_resume(RESUME_FILE_PATH)\n",
    "        print(f\"✅ Resume loaded successfully ({len(resume_text)} characters)\")\n",
    "\n",
    "    # Extract resume features\n",
    "    print(\"🧠 Extracting resume features using LLM...\")\n",
    "    resume_features = resume_processor.extract_resume_features(resume_text)\n",
    "\n",
    "    print(\"✅ Resume features extracted:\")\n",
    "    for key, value in resume_features.items():\n",
    "        if isinstance(value, list):\n",
    "            print(\n",
    "                f\"   {key}: {len(value)} items - {value[:3]}{'...' if len(value) > 3 else ''}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error processing resume: {e}\")\n",
    "    resume_text = \"\"\n",
    "    resume_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 STEP 2: Matching Jobs\n",
      "========================================\n",
      "🔍 Starting job matching process...\n",
      "📝 Prepared 20 job descriptions\n",
      "🧠 Calculating semantic similarities with enhanced TF-IDF...\n",
      "🔤 Calculating keyword similarities...\n",
      "⚙️ Calculating preference scores...\n",
      "✅ Found top 15 job matches using enhanced TF-IDF\n",
      "\n",
      "🏆 TOP 15 JOB MATCHES:\n",
      "==================================================\n",
      "\n",
      "1. Environmental Data Engineer / Machine Learning Engineer at Grassroots Carbon\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Mid-Senior level\n",
      "   🏢 Function: Other\n",
      "   💰 Salary: None\n",
      "   📊 Scores: Final=0.239 | Semantic=0.065 | Keyword=0.093 | Preference=0.625\n",
      "\n",
      "2. Principal Machine Learning Engineer, Recommendation Systems at Launch Potato\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Director\n",
      "   🏢 Function: Engineering and Information Technology\n",
      "   💰 Salary: $160,000.00/yr - $250,000.00/yr\n",
      "   📊 Scores: Final=0.238 | Semantic=0.064 | Keyword=0.095 | Preference=0.625\n",
      "\n",
      "3. Sr Data Scientist - eCommerce at H-E-B\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Mid-Senior level\n",
      "   🏢 Function: Engineering and Information Technology\n",
      "   💰 Salary: None\n",
      "   📊 Scores: Final=0.237 | Semantic=0.063 | Keyword=0.089 | Preference=0.625\n",
      "\n",
      "4. Research Scientist Associate IV, Center for Archaeological Research (CAR) at The University of Texas at San Antonio\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Mid-Senior level\n",
      "   🏢 Function: Other\n",
      "   💰 Salary: $51,000.00/yr - $57,750.00/yr\n",
      "   📊 Scores: Final=0.231 | Semantic=0.058 | Keyword=0.073 | Preference=0.625\n",
      "\n",
      "5. Data Science Engineer I - US at Rackspace Technology\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Not Applicable\n",
      "   🏢 Function: Information Technology\n",
      "   💰 Salary: $69,900.00/yr - $119,460.00/yr\n",
      "   📊 Scores: Final=0.224 | Semantic=0.047 | Keyword=0.065 | Preference=0.625\n",
      "\n",
      "6. AI/Data Engineer – Software Supply Chain Security at Oteemo Inc.\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Mid-Senior level\n",
      "   🏢 Function: Information Technology\n",
      "   💰 Salary: None\n",
      "   📊 Scores: Final=0.223 | Semantic=0.043 | Keyword=0.069 | Preference=0.625\n",
      "\n",
      "7. AI Automation Specialist at Jobot\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Not Applicable\n",
      "   🏢 Function: Engineering and Information Technology\n",
      "   💰 Salary: $100,000.00/yr - $110,000.00/yr\n",
      "   📊 Scores: Final=0.222 | Semantic=0.044 | Keyword=0.063 | Preference=0.625\n",
      "\n",
      "8. Principal ML Engineer, Recommendation Systems at Launch Potato\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Director\n",
      "   🏢 Function: Engineering and Information Technology\n",
      "   💰 Salary: $160,000.00/yr - $250,000.00/yr\n",
      "   📊 Scores: Final=0.214 | Semantic=0.034 | Keyword=0.048 | Preference=0.625\n",
      "\n",
      "9. COMPUTER SCIENTIST - ENGINEER - RESEARCH ENGINEER - Embedded Software Engineer at Southwest Research Institute\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Mid-Senior level\n",
      "   🏢 Function: Engineering and Information Technology\n",
      "   💰 Salary: None\n",
      "   📊 Scores: Final=0.211 | Semantic=0.030 | Keyword=0.040 | Preference=0.625\n",
      "\n",
      "10. Principal ML Engineer, Ad Performance at Launch Potato\n",
      "   📍 Location: San Antonio, TX (N/A)\n",
      "   💼 Level: Director\n",
      "   🏢 Function: Engineering and Information Technology\n",
      "   💰 Salary: $160,000.00/yr - $250,000.00/yr\n",
      "   📊 Scores: Final=0.204 | Semantic=0.020 | Keyword=0.030 | Preference=0.625\n",
      "   ... and 5 more matches\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Match jobs\n",
    "print(\"\\n🎯 STEP 2: Matching Jobs\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if \"jobs_df\" in locals() and len(jobs_df) > 0 and resume_text:\n",
    "    try:\n",
    "        # Perform job matching\n",
    "        top_matches = job_matcher.match_jobs(\n",
    "            resume_features=resume_features,\n",
    "            resume_text=resume_text,\n",
    "            jobs_df=jobs_df,\n",
    "            user_preferences=USER_PREFERENCES,\n",
    "            top_k=TOP_K_MATCHES,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n🏆 TOP {len(top_matches)} JOB MATCHES:\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Display results\n",
    "        for idx, (_, job) in enumerate(top_matches.iterrows(), 1):\n",
    "            print(f\"\\n{idx}. {job['title']} at {job['company']}\")\n",
    "            print(\n",
    "                f\"   📍 Location: {job['location']} ({job.get('work_location', 'N/A')})\"\n",
    "            )\n",
    "            print(f\"   💼 Level: {job.get('level', 'N/A')}\")\n",
    "            print(f\"   🏢 Function: {job.get('job_function', 'N/A')}\")\n",
    "            print(f\"   💰 Salary: {job.get('salary_range', 'N/A')}\")\n",
    "            print(\n",
    "                f\"   📊 Scores: Final={job['final_score']:.3f} | Semantic={job['semantic_score']:.3f} | Keyword={job['keyword_score']:.3f} | Preference={job['preference_score']:.3f}\"\n",
    "            )\n",
    "\n",
    "            if idx >= 10:  # Limit display to top 10\n",
    "                print(f\"   ... and {len(top_matches) - 10} more matches\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during job matching: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"❌ Cannot perform matching - missing jobs data or resume text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DETAILED MATCH ANALYSIS\n",
      "==================================================\n",
      "\n",
      "1. Environmental Data Engineer / Machine Learning Engineer at Grassroots Carbon\n",
      "   Score: 0.239\n",
      "   Analysis:\n",
      "   Unable to generate explanation: \"Input to PromptTemplate is missing variables {'job_info[job_function]', 'resume_features[industries]', 'resume_features[job_titles]', 'job_info[industries]', 'job_info[location]', 'job_info[title]', 'job_info[company]', 'resume_features[skills]', 'job_info[level]', 'resume_features[experience_years]'}.  Expected: ['job_info[company]', 'job_info[industries]', 'job_info[job_function]', 'job_info[level]', 'job_info[location]', 'job_info[title]', 'resume_features[experience_years]', 'resume_features[industries]', 'resume_features[job_titles]', 'resume_features[skills]'] Received: ['job_info', 'resume_features']\\nNote: if you intended {job_info[job_function]} to be part of the string and not a variable, please escape it with double curly braces like: '{{job_info[job_function]}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
      "\n",
      "\n",
      "2. Principal Machine Learning Engineer, Recommendation Systems at Launch Potato\n",
      "   Score: 0.238\n",
      "   Analysis:\n",
      "   Unable to generate explanation: \"Input to PromptTemplate is missing variables {'job_info[job_function]', 'resume_features[industries]', 'resume_features[job_titles]', 'job_info[industries]', 'job_info[location]', 'job_info[title]', 'job_info[company]', 'resume_features[skills]', 'job_info[level]', 'resume_features[experience_years]'}.  Expected: ['job_info[company]', 'job_info[industries]', 'job_info[job_function]', 'job_info[level]', 'job_info[location]', 'job_info[title]', 'resume_features[experience_years]', 'resume_features[industries]', 'resume_features[job_titles]', 'resume_features[skills]'] Received: ['job_info', 'resume_features']\\nNote: if you intended {job_info[job_function]} to be part of the string and not a variable, please escape it with double curly braces like: '{{job_info[job_function]}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
      "\n",
      "\n",
      "3. Sr Data Scientist - eCommerce at H-E-B\n",
      "   Score: 0.237\n",
      "   Analysis:\n",
      "   Unable to generate explanation: \"Input to PromptTemplate is missing variables {'job_info[job_function]', 'resume_features[industries]', 'resume_features[job_titles]', 'job_info[industries]', 'job_info[location]', 'job_info[title]', 'job_info[company]', 'resume_features[skills]', 'job_info[level]', 'resume_features[experience_years]'}.  Expected: ['job_info[company]', 'job_info[industries]', 'job_info[job_function]', 'job_info[level]', 'job_info[location]', 'job_info[title]', 'resume_features[experience_years]', 'resume_features[industries]', 'resume_features[job_titles]', 'resume_features[skills]'] Received: ['job_info', 'resume_features']\\nNote: if you intended {job_info[job_function]} to be part of the string and not a variable, please escape it with double curly braces like: '{{job_info[job_function]}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
      "\n",
      "\n",
      "4. Research Scientist Associate IV, Center for Archaeological Research (CAR) at The University of Texas at San Antonio\n",
      "   Score: 0.231\n",
      "   Analysis:\n",
      "   Unable to generate explanation: \"Input to PromptTemplate is missing variables {'job_info[job_function]', 'resume_features[industries]', 'resume_features[job_titles]', 'job_info[industries]', 'job_info[location]', 'job_info[title]', 'job_info[company]', 'resume_features[skills]', 'job_info[level]', 'resume_features[experience_years]'}.  Expected: ['job_info[company]', 'job_info[industries]', 'job_info[job_function]', 'job_info[level]', 'job_info[location]', 'job_info[title]', 'resume_features[experience_years]', 'resume_features[industries]', 'resume_features[job_titles]', 'resume_features[skills]'] Received: ['job_info', 'resume_features']\\nNote: if you intended {job_info[job_function]} to be part of the string and not a variable, please escape it with double curly braces like: '{{job_info[job_function]}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
      "\n",
      "\n",
      "5. Data Science Engineer I - US at Rackspace Technology\n",
      "   Score: 0.224\n",
      "   Analysis:\n",
      "   Unable to generate explanation: \"Input to PromptTemplate is missing variables {'job_info[job_function]', 'resume_features[industries]', 'resume_features[job_titles]', 'job_info[industries]', 'job_info[location]', 'job_info[title]', 'job_info[company]', 'resume_features[skills]', 'job_info[level]', 'resume_features[experience_years]'}.  Expected: ['job_info[company]', 'job_info[industries]', 'job_info[job_function]', 'job_info[level]', 'job_info[location]', 'job_info[title]', 'resume_features[experience_years]', 'resume_features[industries]', 'resume_features[job_titles]', 'resume_features[skills]'] Received: ['job_info', 'resume_features']\\nNote: if you intended {job_info[job_function]} to be part of the string and not a variable, please escape it with double curly braces like: '{{job_info[job_function]}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate detailed explanations for top matches\n",
    "def generate_match_explanation(\n",
    "    job_row: pd.Series, resume_features: Dict[str, Any]\n",
    ") -> str:\n",
    "    \"\"\"Generate detailed explanation for why a job matches\"\"\"\n",
    "\n",
    "    explanation_prompt = PromptTemplate(\n",
    "        input_variables=[\"job_info\", \"resume_features\"],\n",
    "        template=\"\"\"\n",
    "        Analyze why this job is a good match for the candidate and provide a detailed explanation.\n",
    "        \n",
    "        Job Information:\n",
    "        - Title: {job_info[title]}\n",
    "        - Company: {job_info[company]}\n",
    "        - Location: {job_info[location]}\n",
    "        - Level: {job_info[level]}\n",
    "        - Function: {job_info[job_function]}\n",
    "        - Industry: {job_info[industries]}\n",
    "        \n",
    "        Candidate Profile:\n",
    "        - Skills: {resume_features[skills]}\n",
    "        - Experience: {resume_features[experience_years]} years\n",
    "        - Previous Roles: {resume_features[job_titles]}\n",
    "        - Industries: {resume_features[industries]}\n",
    "        \n",
    "        Provide a concise explanation (2-3 sentences) covering:\n",
    "        1. Key skill alignments\n",
    "        2. Experience level match\n",
    "        3. Industry/function relevance\n",
    "        4. Any potential concerns or gaps\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        job_info = {\n",
    "            \"title\": job_row.get(\"title\", \"N/A\"),\n",
    "            \"company\": job_row.get(\"company\", \"N/A\"),\n",
    "            \"location\": job_row.get(\"location\", \"N/A\"),\n",
    "            \"level\": job_row.get(\"level\", \"N/A\"),\n",
    "            \"job_function\": job_row.get(\"job_function\", \"N/A\"),\n",
    "            \"industries\": job_row.get(\"industries\", \"N/A\"),\n",
    "        }\n",
    "\n",
    "        chain = explanation_prompt | llm\n",
    "        explanation = chain.invoke(\n",
    "            {\"job_info\": job_info, \"resume_features\": resume_features}\n",
    "        )\n",
    "\n",
    "        return explanation.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Unable to generate explanation: {e}\"\n",
    "\n",
    "\n",
    "# Generate explanations for top 5 matches\n",
    "if \"top_matches\" in locals() and len(top_matches) > 0:\n",
    "    print(\"\\n🔍 DETAILED MATCH ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for idx, (_, job) in enumerate(top_matches.head(5).iterrows(), 1):\n",
    "        print(f\"\\n{idx}. {job['title']} at {job['company']}\")\n",
    "        print(f\"   Score: {job['final_score']:.3f}\")\n",
    "        print(\"   Analysis:\")\n",
    "\n",
    "        explanation = generate_match_explanation(job, resume_features)\n",
    "        # Format explanation with proper indentation\n",
    "        for line in explanation.split(\"\\n\"):\n",
    "            if line.strip():\n",
    "                print(f\"   {line.strip()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Results exported to: job_matches_20250907_220126.csv\n",
      "📈 Summary Statistics:\n",
      "   Total matches analyzed: 20\n",
      "   Top matches returned: 15\n",
      "   Average final score: 0.213\n",
      "   Score range: 0.148 - 0.239\n",
      "\n",
      "📋 Top matches by job function:\n",
      "   Engineering and Information Technology: 8 matches\n",
      "   Other: 3 matches\n",
      "   Information Technology: 3 matches\n",
      "   Engineering: 1 matches\n"
     ]
    }
   ],
   "source": [
    "# Export results to CSV\n",
    "if \"top_matches\" in locals() and len(top_matches) > 0:\n",
    "    # Prepare export data - check which columns actually exist\n",
    "    available_columns = top_matches.columns.tolist()\n",
    "\n",
    "    # Define desired export columns with correct names\n",
    "    desired_export_columns = [\n",
    "        \"company\",\n",
    "        \"title\",\n",
    "        \"location\",\n",
    "        \"work_location_type\",  # Fixed: was 'work_location'\n",
    "        \"level\",\n",
    "        \"salary_range\",\n",
    "        \"employment_type\",\n",
    "        \"job_function\",\n",
    "        \"industries\",\n",
    "        \"final_score\",\n",
    "        \"semantic_score\",\n",
    "        \"keyword_score\",\n",
    "        \"preference_score\",\n",
    "        \"job_posting_link\",\n",
    "        \"posted_time\",\n",
    "    ]\n",
    "\n",
    "    # Only include columns that actually exist in the dataframe\n",
    "    export_columns = [col for col in desired_export_columns if col in available_columns]\n",
    "\n",
    "    # Print info about missing columns\n",
    "    missing_columns = [\n",
    "        col for col in desired_export_columns if col not in available_columns\n",
    "    ]\n",
    "    if missing_columns:\n",
    "        print(f\"⚠️ Missing columns (will be skipped): {missing_columns}\")\n",
    "\n",
    "    export_df = top_matches[export_columns].copy()\n",
    "\n",
    "    # Round scores for better readability\n",
    "    score_columns = [\n",
    "        \"final_score\",\n",
    "        \"semantic_score\",\n",
    "        \"keyword_score\",\n",
    "        \"preference_score\",\n",
    "    ]\n",
    "    for col in score_columns:\n",
    "        if col in export_df.columns:\n",
    "            export_df[col] = export_df[col].round(4)\n",
    "\n",
    "    # Export to CSV\n",
    "    output_file = f\"job_matches_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    export_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"📊 Results exported to: {output_file}\")\n",
    "    print(f\"📈 Summary Statistics:\")\n",
    "    print(f\"   Total matches analyzed: {len(jobs_df)}\")\n",
    "    print(f\"   Top matches returned: {len(top_matches)}\")\n",
    "    print(f\"   Average final score: {top_matches['final_score'].mean():.3f}\")\n",
    "    print(\n",
    "        f\"   Score range: {top_matches['final_score'].min():.3f} - {top_matches['final_score'].max():.3f}\"\n",
    "    )\n",
    "\n",
    "    # Show distribution by job function\n",
    "    print(f\"\\n📋 Top matches by job function:\")\n",
    "    function_counts = top_matches[\"job_function\"].value_counts().head(5)\n",
    "    for func, count in function_counts.items():\n",
    "        print(f\"   {func}: {count} matches\")\n",
    "else:\n",
    "    print(\"❌ No results to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Refinement function ready (uncomment to use)\n"
     ]
    }
   ],
   "source": [
    "# Function to refine search based on feedback\n",
    "def refine_search(\n",
    "    original_matches: pd.DataFrame, feedback: Dict[str, Any]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Refine job matches based on user feedback\"\"\"\n",
    "\n",
    "    refined_matches = original_matches.copy()\n",
    "\n",
    "    # Apply additional filters based on feedback\n",
    "    if \"exclude_companies\" in feedback:\n",
    "        for company in feedback[\"exclude_companies\"]:\n",
    "            refined_matches = refined_matches[\n",
    "                ~refined_matches[\"company\"].str.contains(company, case=False, na=False)\n",
    "            ]\n",
    "\n",
    "    if \"focus_on_functions\" in feedback:\n",
    "        function_filter = \"|\".join(feedback[\"focus_on_functions\"])\n",
    "        refined_matches = refined_matches[\n",
    "            refined_matches[\"job_function\"].str.contains(\n",
    "                function_filter, case=False, na=False\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    if \"min_score\" in feedback:\n",
    "        refined_matches = refined_matches[\n",
    "            refined_matches[\"final_score\"] >= feedback[\"min_score\"]\n",
    "        ]\n",
    "\n",
    "    return refined_matches.head(10)\n",
    "\n",
    "\n",
    "# Example refinement (uncomment and modify as needed)\n",
    "# REFINEMENT_FEEDBACK = {\n",
    "#     'exclude_companies': ['Company A', 'Company B'],\n",
    "#     'focus_on_functions': ['Information Technology', 'Engineering'],\n",
    "#     'min_score': 0.3\n",
    "# }\n",
    "\n",
    "# if 'top_matches' in locals():\n",
    "#     refined_matches = refine_search(top_matches, REFINEMENT_FEEDBACK)\n",
    "#     print(f\"🔄 Refined search returned {len(refined_matches)} matches\")\n",
    "\n",
    "print(\"✅ Refinement function ready (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 JOB MATCHING COMPLETE!\n",
      "==================================================\n",
      "✅ Successfully matched 15 jobs to your resume\n",
      "📊 Results exported to CSV file\n",
      "🎯 Top match: Environmental Data Engineer / Machine Learning Engineer at Grassroots Carbon\n",
      "📈 Best score: 0.239\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "1. Review the detailed match analysis above\n",
      "2. Check the exported CSV file for complete results\n",
      "3. Visit job posting links for positions of interest\n",
      "4. Customize USER_PREFERENCES and re-run for different results\n",
      "5. Use the refinement section to filter results further\n",
      "\n",
      "💡 TIP: Modify the USER_PREFERENCES section and re-run cells 6-8 to get different results!\n",
      "🔧 For technical issues, check that Ollama is running: ollama serve\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🎉 JOB MATCHING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if \"top_matches\" in locals() and len(top_matches) > 0:\n",
    "    print(f\"✅ Successfully matched {len(top_matches)} jobs to your resume\")\n",
    "    print(f\"📊 Results exported to CSV file\")\n",
    "    print(\n",
    "        f\"🎯 Top match: {top_matches.iloc[0]['title']} at {top_matches.iloc[0]['company']}\"\n",
    "    )\n",
    "    print(f\"📈 Best score: {top_matches.iloc[0]['final_score']:.3f}\")\n",
    "\n",
    "    print(f\"\\n🚀 NEXT STEPS:\")\n",
    "    print(f\"1. Review the detailed match analysis above\")\n",
    "    print(f\"2. Check the exported CSV file for complete results\")\n",
    "    print(f\"3. Visit job posting links for positions of interest\")\n",
    "    print(f\"4. Customize USER_PREFERENCES and re-run for different results\")\n",
    "    print(f\"5. Use the refinement section to filter results further\")\n",
    "else:\n",
    "    print(\"❌ No matches found. Consider:\")\n",
    "    print(\"   - Checking your resume file path\")\n",
    "    print(\"   - Verifying database connection\")\n",
    "    print(\"   - Adjusting user preferences\")\n",
    "    print(\"   - Ensuring Ollama is running with llama3.2 model\")\n",
    "\n",
    "print(\n",
    "    f\"\\n💡 TIP: Modify the USER_PREFERENCES section and re-run cells 6-8 to get different results!\"\n",
    ")\n",
    "print(f\"🔧 For technical issues, check that Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded environment variables from .env file\n",
      "✅ Hugging Face token found, configuring for authenticated access\n",
      "🔧 Hugging Face environment configured\n",
      "🖥️ PyTorch device: cuda\n",
      "🖥️ PyTorch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-09-07 22:01:27,320 - huggingface_hub._login - WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-09-07 22:01:27,323 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-09-07 22:01:27,320 - huggingface_hub._login - WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-09-07 22:01:27,323 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-09-07 22:01:27,404 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n",
      "2025-09-07 22:01:27,404 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged in to Hugging Face Hub\n",
      "🔄 Loading sentence transformer model...\n",
      "🌐 Connection error - check firewall/proxy settings\n",
      "\n",
      "📊 Using TF-IDF similarity (works great for job matching!)\n",
      "\n",
      "💡 To use sentence transformers, try ONE of these:\n",
      "   1. Set SKIP_SENTENCE_TRANSFORMERS=true in .env (recommended)\n",
      "   2. Run in terminal: pip install --upgrade sentence-transformers transformers huggingface-hub\n",
      "   3. Check proxy/firewall settings if behind corporate network\n",
      "\n",
      "✨ TF-IDF similarity is highly effective for job matching, so don't worry!\n",
      "🌐 Connection error - check firewall/proxy settings\n",
      "\n",
      "📊 Using TF-IDF similarity (works great for job matching!)\n",
      "\n",
      "💡 To use sentence transformers, try ONE of these:\n",
      "   1. Set SKIP_SENTENCE_TRANSFORMERS=true in .env (recommended)\n",
      "   2. Run in terminal: pip install --upgrade sentence-transformers transformers huggingface-hub\n",
      "   3. Check proxy/firewall settings if behind corporate network\n",
      "\n",
      "✨ TF-IDF similarity is highly effective for job matching, so don't worry!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Setup and Model Initialization - MUST RUN BEFORE OTHER CELLS\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "env_file = Path(\"../.env\")\n",
    "if env_file.exists():\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv(env_file)\n",
    "    print(\"✅ Loaded environment variables from .env file\")\n",
    "else:\n",
    "    print(\"⚠️ No .env file found, using default settings\")\n",
    "\n",
    "# Check if we should skip sentence transformers entirely\n",
    "skip_sentence_transformers = (\n",
    "    os.getenv(\"SKIP_SENTENCE_TRANSFORMERS\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "if skip_sentence_transformers:\n",
    "    print(\"🚫 SKIP_SENTENCE_TRANSFORMERS=true - Skipping sentence transformer setup\")\n",
    "    sentence_transformer_available = False\n",
    "else:\n",
    "    # Check if we have a Hugging Face token\n",
    "    hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "    if hf_token and hf_token.startswith(\"hf_\"):\n",
    "        print(\"✅ Hugging Face token found, configuring for authenticated access\")\n",
    "        # Set the token for huggingface-hub\n",
    "        os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token  # Alternative env var name\n",
    "\n",
    "        # Enable online mode with authentication\n",
    "        os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "        os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "        os.environ[\"HF_DATASETS_OFFLINE\"] = \"0\"\n",
    "    else:\n",
    "        print(\"⚠️ No Hugging Face token found, forcing offline mode\")\n",
    "        os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "        os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "        os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "\n",
    "    # Always disable telemetry\n",
    "    os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "    # Set local cache directories\n",
    "    models_dir = Path(\"./models\")\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = str(models_dir / \"transformers_cache\")\n",
    "    os.environ[\"HF_HOME\"] = str(models_dir / \"hf_cache\")\n",
    "    os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = str(models_dir / \"sentence_transformers\")\n",
    "\n",
    "    # Suppress transformer warnings\n",
    "    try:\n",
    "        from transformers import logging as transformers_logging\n",
    "\n",
    "        transformers_logging.set_verbosity_error()\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    print(\"🔧 Hugging Face environment configured\")\n",
    "\n",
    "    # Test sentence transformer availability\n",
    "    sentence_transformer_available = False\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"🖥️ PyTorch device: {device}\")\n",
    "\n",
    "        # Configure huggingface_hub with token\n",
    "        if hf_token:\n",
    "            try:\n",
    "                from huggingface_hub import login\n",
    "\n",
    "                login(token=hf_token, add_to_git_credential=False)\n",
    "                print(\"✅ Logged in to Hugging Face Hub\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not login to HF Hub: {e}\")\n",
    "\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        print(\"🔄 Loading sentence transformer model...\")\n",
    "\n",
    "        # Try loading with explicit cache directory and simpler model name\n",
    "        try:\n",
    "            # Use just the model name without organization prefix\n",
    "            model = SentenceTransformer(\n",
    "                \"all-MiniLM-L6-v2\",\n",
    "                device=device,\n",
    "                cache_folder=str(models_dir / \"sentence_transformers\"),\n",
    "            )\n",
    "\n",
    "            # Test the model\n",
    "            test_embedding = model.encode(\"test sentence\", show_progress_bar=False)\n",
    "            if test_embedding is not None and len(test_embedding) > 0:\n",
    "                sentence_transformer_available = True\n",
    "                print(\n",
    "                    f\"✅ Sentence transformer loaded successfully! Embedding size: {len(test_embedding)}\"\n",
    "                )\n",
    "\n",
    "            del model  # Free memory\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if \"connection error\" in error_msg.lower():\n",
    "                print(\"🌐 Connection error - check firewall/proxy settings\")\n",
    "            elif \"401\" in error_msg or \"unauthorized\" in error_msg.lower():\n",
    "                print(\"🔑 Authentication error - check HF token validity\")\n",
    "            else:\n",
    "                print(f\"⚠️ Failed to load model: {error_msg[:200]}...\")\n",
    "            sentence_transformer_available = False\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"⚠️ Missing dependency: {e}\")\n",
    "        print(\"💡 Install with: pip install sentence-transformers\")\n",
    "        sentence_transformer_available = False\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Unexpected error: {e}\")\n",
    "        sentence_transformer_available = False\n",
    "\n",
    "# Final status\n",
    "if sentence_transformer_available:\n",
    "    print(\"\\n🎉 SUCCESS! Sentence transformers are ready to use!\")\n",
    "else:\n",
    "    print(\"\\n📊 Using TF-IDF similarity (works great for job matching!)\")\n",
    "    print(\"\\n💡 To use sentence transformers, try ONE of these:\")\n",
    "    print(\"   1. Set SKIP_SENTENCE_TRANSFORMERS=true in .env (recommended)\")\n",
    "    print(\n",
    "        \"   2. Run in terminal: pip install --upgrade sentence-transformers transformers huggingface-hub\"\n",
    "    )\n",
    "    print(\"   3. Check proxy/firewall settings if behind corporate network\")\n",
    "    print(\n",
    "        \"\\n✨ TF-IDF similarity is highly effective for job matching, so don't worry!\"\n",
    "    )\n",
    "\n",
    "# Store the availability status for other cells\n",
    "globals()[\"SENTENCE_TRANSFORMER_AVAILABLE\"] = sentence_transformer_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ python-dotenv already installed\n"
     ]
    }
   ],
   "source": [
    "# Install python-dotenv if not available (run once)\n",
    "try:\n",
    "    import dotenv\n",
    "\n",
    "    print(\"✅ python-dotenv already installed\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing python-dotenv...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
    "    print(\"✅ python-dotenv installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Uncomment the code above if you want to manually download models\n"
     ]
    }
   ],
   "source": [
    "# Manual Model Download (run this if automatic loading fails)\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_sentence_transformer_model(model_name: str, local_dir: Path):\n",
    "    \"\"\"Manually download a sentence transformer model\"\"\"\n",
    "    try:\n",
    "        print(f\"📥 Downloading {model_name} to {local_dir}...\")\n",
    "\n",
    "        # Create directory\n",
    "        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Download using huggingface-cli\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"huggingface_hub.commands.huggingface_cli\",\n",
    "            \"download\",\n",
    "            model_name,\n",
    "            \"--local-dir\",\n",
    "            str(local_dir),\n",
    "            \"--local-dir-use-symlinks\",\n",
    "            \"False\",\n",
    "        ]\n",
    "\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Successfully downloaded {model_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Failed to download {model_name}: {result.stderr}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading {model_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Uncomment and run this section if you want to manually download models\n",
    "# models_to_download = [\n",
    "#     (\"sentence-transformers/all-MiniLM-L6-v2\", \"./models/all-MiniLM-L6-v2\"),\n",
    "#     (\"sentence-transformers/paraphrase-MiniLM-L3-v2\", \"./models/paraphrase-MiniLM-L3-v2\")\n",
    "# ]\n",
    "\n",
    "# for model_name, local_path in models_to_download:\n",
    "#     download_sentence_transformer_model(model_name, Path(local_path))\n",
    "\n",
    "print(\"💡 Uncomment the code above if you want to manually download models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Checking Internet Connectivity...\n",
      "==================================================\n",
      "✅ Google DNS reachable - Internet connection available\n",
      "✅ Hugging Face Hub accessible (https://huggingface.co)\n",
      "✅ Hugging Face Hub accessible (https://huggingface.co)\n",
      "🔑 Hugging Face API requires authentication (401)\n",
      "🔑 Hugging Face API requires authentication (401)\n",
      "✅ GitHub accessible (https://github.com)\n",
      "\n",
      "🔑 Testing Hugging Face API with token...\n",
      "✅ GitHub accessible (https://github.com)\n",
      "\n",
      "🔑 Testing Hugging Face API with token...\n",
      "✅ Hugging Face API accessible with token\n",
      "==================================================\n",
      "\n",
      "✅ Internet connection seems OK\n",
      "   If sentence transformers still fail, try:\n",
      "   1. Verify your HF token is valid\n",
      "   2. Set SKIP_SENTENCE_TRANSFORMERS=true in .env\n",
      "✅ Hugging Face API accessible with token\n",
      "==================================================\n",
      "\n",
      "✅ Internet connection seems OK\n",
      "   If sentence transformers still fail, try:\n",
      "   1. Verify your HF token is valid\n",
      "   2. Set SKIP_SENTENCE_TRANSFORMERS=true in .env\n"
     ]
    }
   ],
   "source": [
    "# Check Internet Connection and Hugging Face Access\n",
    "import socket\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import ssl\n",
    "\n",
    "\n",
    "def check_internet_connection():\n",
    "    \"\"\"Check internet connectivity and Hugging Face access\"\"\"\n",
    "\n",
    "    # Test general internet connectivity\n",
    "    test_sites = [\n",
    "        (\"Google DNS\", \"8.8.8.8\", 53),\n",
    "        (\"Cloudflare DNS\", \"1.1.1.1\", 53),\n",
    "    ]\n",
    "\n",
    "    internet_available = False\n",
    "    for name, host, port in test_sites:\n",
    "        try:\n",
    "            socket.create_connection((host, port), timeout=5)\n",
    "            print(f\"✅ {name} reachable - Internet connection available\")\n",
    "            internet_available = True\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not internet_available:\n",
    "        print(\"❌ No internet connection detected\")\n",
    "        return False\n",
    "\n",
    "    # Test HTTPS connectivity to specific sites\n",
    "    test_urls = [\n",
    "        (\"Hugging Face Hub\", \"https://huggingface.co\"),\n",
    "        (\"Hugging Face API\", \"https://api-inference.huggingface.co\"),\n",
    "        (\"GitHub\", \"https://github.com\"),\n",
    "    ]\n",
    "\n",
    "    # Create SSL context\n",
    "    ssl_context = ssl.create_default_context()\n",
    "\n",
    "    for name, url in test_urls:\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            with urllib.request.urlopen(\n",
    "                req, timeout=10, context=ssl_context\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    print(f\"✅ {name} accessible ({url})\")\n",
    "                else:\n",
    "                    print(f\"⚠️ {name} returned status {response.status}\")\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 401:\n",
    "                print(f\"🔑 {name} requires authentication (401)\")\n",
    "            else:\n",
    "                print(f\"❌ {name} HTTP error: {e.code}\")\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"❌ {name} connection error: {e.reason}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {name} error: {type(e).__name__}: {str(e)[:100]}\")\n",
    "\n",
    "    # Test with Hugging Face token if available\n",
    "    import os\n",
    "\n",
    "    hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "    if hf_token and hf_token.startswith(\"hf_\"):\n",
    "        print(\"\\n🔑 Testing Hugging Face API with token...\")\n",
    "        try:\n",
    "            req = urllib.request.Request(\n",
    "                \"https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {hf_token}\",\n",
    "                    \"User-Agent\": \"Mozilla/5.0\",\n",
    "                },\n",
    "            )\n",
    "            with urllib.request.urlopen(\n",
    "                req, timeout=10, context=ssl_context\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    print(\"✅ Hugging Face API accessible with token\")\n",
    "                    return True\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"❌ Hugging Face API error with token: {e.code} - {e.reason}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to access Hugging Face API: {e}\")\n",
    "\n",
    "    return internet_available\n",
    "\n",
    "\n",
    "# Run the connection check\n",
    "print(\"🌐 Checking Internet Connectivity...\")\n",
    "print(\"=\" * 50)\n",
    "connection_ok = check_internet_connection()\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not connection_ok:\n",
    "    print(\"\\n💡 Connection Issues Detected!\")\n",
    "    print(\"   1. Check your internet connection\")\n",
    "    print(\"   2. Check if you're behind a proxy/firewall\")\n",
    "    print(\"   3. Try: export SKIP_SENTENCE_TRANSFORMERS=true\")\n",
    "else:\n",
    "    print(\"\\n✅ Internet connection seems OK\")\n",
    "    print(\"   If sentence transformers still fail, try:\")\n",
    "    print(\"   1. Verify your HF token is valid\")\n",
    "    print(\"   2. Set SKIP_SENTENCE_TRANSFORMERS=true in .env\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-job-finder-Y_k-9c-5-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
